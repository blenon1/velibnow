{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83458767",
   "metadata": {},
   "source": [
    "# Fonctions d'obtention des données\n",
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbcb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from config import URL_HOLIDAYS, URL_SCHOOL, URL_METEO_HOURLY\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66032f0",
   "metadata": {},
   "source": [
    "## Class de lecture des données velib historiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd96b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelibCsvReader:\n",
    "    def __init__(self):\n",
    "        self.file_path = \"./data/dataset/historique_stations.csv\"\n",
    "\n",
    "    def read_dataframe(self):\n",
    "        \"\"\"Combine status + info sur les stations\"\"\"\n",
    "        dataframe = pd.read_csv(\n",
    "            filepath_or_buffer=self.file_path,\n",
    "            sep=\",\"\n",
    "        )\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfbf7c",
   "metadata": {},
   "source": [
    "## Class API pour données sur les vacances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01808ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolidaysAPI:\n",
    "    \"\"\"\n",
    "    Récupération des jours fériés et des vacances scolaires françaises.\n",
    "    \"\"\"\n",
    "\n",
    "    def fetch_public_holidays(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = requests.get(URL_HOLIDAYS, timeout=10).json()\n",
    "            df = pd.DataFrame(list(data.items()), columns=[\"date\", \"holiday_name\"])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erreur API jours fériés : {e}\")\n",
    "\n",
    "    def fetch_school_vacations(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = requests.get(URL_SCHOOL, timeout=10).json()\n",
    "            results = data.get(\"results\", [])\n",
    "            df = pd.DataFrame(results)\n",
    "            if not df.empty:\n",
    "                df = df[[\"start_date\", \"end_date\", \"zones\", \"description\"]]\n",
    "                df[\"start_date\"] = pd.to_datetime(df[\"start_date\"])\n",
    "                df[\"end_date\"] = pd.to_datetime(df[\"end_date\"])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erreur API vacances scolaires : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee9993",
   "metadata": {},
   "source": [
    "## Class API pour les données météo historiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7b4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "class WeatherCsvReader:\n",
    "    def __init__(self, file_path: str = \"./data/dataset/historical_meteo.csv\"):\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        # Mapping colonnes brutes -> colonnes standardisées\n",
    "        self.WEATHER_COL_MAP = {\n",
    "            \"AAAAMMJJHH\": \"timestamp\",\n",
    "            \"NUM_POSTE\":  \"station_id\",\n",
    "            \"NOM_USUEL\":  \"station_name\",\n",
    "            \"LAT\":        \"lat_deg\",\n",
    "            \"LON\":        \"lon_deg\",\n",
    "            \"ALTI\":       \"alt_m\",\n",
    "            \"RR1\":        \"precip_mm\",\n",
    "            \"DRR1\":       \"precip_dur_min\",\n",
    "            \"T\":          \"temp_c\",\n",
    "            \"TD\":         \"dewpoint_c\",\n",
    "            \"U\":          \"humidity_rel_pct\",\n",
    "            \"FF\":         \"wind_speed_10m_ms\",\n",
    "            \"DD\":         \"wind_dir_deg\",\n",
    "            \"FXI\":        \"wind_gust_10m_ms\",\n",
    "            \"PSTAT\":      \"pressure_hpa_station\",\n",
    "            \"PMER\":       \"pressure_hpa_sea\",\n",
    "\n",
    "            \"N\":          \"cloud_oktas\",\n",
    "            \"INS\":        \"insolation_min\",            # plan B si N absent\n",
    "            \"GLO\":        \"global_radiation_j_cm2\",    # plan B bis\n",
    "            \"WW\":         \"wmo_present_weather\",       # optionnel mais utile\n",
    "        }\n",
    "\n",
    "        # Schéma final (cible) — timestamp est parsé à part\n",
    "        self.WEATHER_DTYPES: Dict[str, str] = {\n",
    "            \"station_id\":          \"string\",\n",
    "            \"station_name\":        \"string\",\n",
    "            \"lat_deg\":             \"float64\",\n",
    "            \"lon_deg\":             \"float64\",\n",
    "            \"alt_m\":               \"Int64\",\n",
    "            \"timestamp\":           \"datetime64[ns]\",\n",
    "            \"precip_mm\":           \"float64\",\n",
    "            \"precip_dur_min\":      \"Int64\",\n",
    "            \"temp_c\":              \"float64\",\n",
    "            \"dewpoint_c\":          \"float64\",\n",
    "            \"humidity_rel_pct\":    \"float64\",\n",
    "            \"wind_speed_10m_ms\":   \"float64\",\n",
    "            \"wind_dir_deg\":        \"float64\",\n",
    "            \"wind_gust_10m_ms\":    \"float64\",\n",
    "            \"pressure_hpa\":        \"float64\",\n",
    "            \n",
    "            \"cloud_oktas\":   \"float64\",\n",
    "            \"insolation_min\":        \"float64\",\n",
    "            \"global_radiation_j_cm2\":    \"float64\",\n",
    "            \"wmo_present_weather\":        \"float64\",\n",
    "        }\n",
    "\n",
    "    def read_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Lecture brute du CSV (séparateur ';').\"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(self.file_path, sep=\";\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erreur API météo : {e}\")\n",
    "\n",
    "    def _select_rename(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Garde uniquement les colonnes connues puis les renomme (sans boucles).\"\"\"\n",
    "        cols_src = df.columns.intersection(self.WEATHER_COL_MAP.keys())\n",
    "        return df.loc[:, cols_src].rename(columns=self.WEATHER_COL_MAP)\n",
    "\n",
    "    def read_standardized(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Pipeline vectorisé :\n",
    "        - select/rename\n",
    "        - parse timestamp (AAAAMMJJHH)\n",
    "        - coalesce pression (PSTAT prioritaire sur PMER)\n",
    "        - cast global selon schéma\n",
    "        - ordre de colonnes propre\n",
    "        \"\"\"\n",
    "        df_raw = self.read_dataframe()\n",
    "        df = self._select_rename(df_raw)\n",
    "\n",
    "        # Parse timestamp (format AAAAMMJJHH)\n",
    "        df = df.assign(\n",
    "            timestamp=pd.to_datetime(\n",
    "                df[\"timestamp\"].astype(\"string\"),\n",
    "                format=\"%Y%m%d%H\",\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Coalesce pression (priorité station -> mer) si colonnes présentes\n",
    "        # (si absentes, bfill s'applique sur colonnes manquantes sans boucle)\n",
    "        pressure_sources = df.filter(items=[\"pressure_hpa_station\", \"pressure_hpa_sea\"])\n",
    "        if not pressure_sources.empty:\n",
    "            df[\"pressure_hpa\"] = pressure_sources.bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "        # Ordre + cast en une seule passe (les colonnes manquantes seront ajoutées vides)\n",
    "        df = (\n",
    "            df\n",
    "            .reindex(columns=self.WEATHER_DTYPES.keys())  # ordre final\n",
    "            .astype(self.WEATHER_DTYPES, errors=\"ignore\")  # cast \"df.cast(schema)\" version pandas\n",
    "        )\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4119355",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427cb21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données Vélos :\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "capacity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "available_mechanical",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "available_electrical",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "station_geo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "operative",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "5188a36f-f2d0-4251-9b44-6a0bee0d0c73",
       "rows": [
        [
         "0",
         "2020-11-26T12:59Z",
         "35",
         "4",
         "5",
         "Benjamin Godard - Victor Hugo",
         "48.86598,2.27572",
         "True"
        ],
        [
         "1",
         "2020-11-26T12:59Z",
         "55",
         "23",
         "4",
         "André Mazet - Saint-André des Arts",
         "48.85376,2.33910",
         "True"
        ],
        [
         "2",
         "2020-11-26T12:59Z",
         "20",
         "0",
         "0",
         "Charonne - Robert et Sonia Delauney",
         "48.85591,2.39257",
         "True"
        ],
        [
         "3",
         "2020-11-26T12:59Z",
         "21",
         "0",
         "1",
         "Toudouze - Clauzel",
         "48.87930,2.33736",
         "True"
        ],
        [
         "4",
         "2020-11-26T12:59Z",
         "30",
         "3",
         "1",
         "Mairie du 12ème",
         "48.84086,2.38755",
         "True"
        ],
        [
         "5",
         "2020-11-26T12:59Z",
         "46",
         "18",
         "10",
         "Harpe - Saint-Germain",
         "48.85152,2.34367",
         "True"
        ],
        [
         "6",
         "2020-11-26T12:59Z",
         "60",
         "5",
         "2",
         "Jourdan - Stade Charléty",
         "48.81943,2.34334",
         "True"
        ],
        [
         "7",
         "2020-11-26T12:59Z",
         "40",
         "15",
         "1",
         "Jouffroy d'Abbans - Wagram",
         "48.88197,2.30113",
         "True"
        ],
        [
         "8",
         "2020-11-26T12:59Z",
         "39",
         "12",
         "2",
         "Guersant - Gouvion-Saint-Cyr",
         "48.88288,2.28767",
         "True"
        ],
        [
         "9",
         "2020-11-26T12:59Z",
         "60",
         "2",
         "2",
         "Alibert - Jemmapes",
         "48.87104,2.36610",
         "True"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>capacity</th>\n",
       "      <th>available_mechanical</th>\n",
       "      <th>available_electrical</th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_geo</th>\n",
       "      <th>operative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Benjamin Godard - Victor Hugo</td>\n",
       "      <td>48.86598,2.27572</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>André Mazet - Saint-André des Arts</td>\n",
       "      <td>48.85376,2.33910</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Charonne - Robert et Sonia Delauney</td>\n",
       "      <td>48.85591,2.39257</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Toudouze - Clauzel</td>\n",
       "      <td>48.87930,2.33736</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mairie du 12ème</td>\n",
       "      <td>48.84086,2.38755</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>Harpe - Saint-Germain</td>\n",
       "      <td>48.85152,2.34367</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Jourdan - Stade Charléty</td>\n",
       "      <td>48.81943,2.34334</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Jouffroy d'Abbans - Wagram</td>\n",
       "      <td>48.88197,2.30113</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Guersant - Gouvion-Saint-Cyr</td>\n",
       "      <td>48.88288,2.28767</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Alibert - Jemmapes</td>\n",
       "      <td>48.87104,2.36610</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                time  capacity  available_mechanical  available_electrical  \\\n",
       "0  2020-11-26T12:59Z        35                     4                     5   \n",
       "1  2020-11-26T12:59Z        55                    23                     4   \n",
       "2  2020-11-26T12:59Z        20                     0                     0   \n",
       "3  2020-11-26T12:59Z        21                     0                     1   \n",
       "4  2020-11-26T12:59Z        30                     3                     1   \n",
       "5  2020-11-26T12:59Z        46                    18                    10   \n",
       "6  2020-11-26T12:59Z        60                     5                     2   \n",
       "7  2020-11-26T12:59Z        40                    15                     1   \n",
       "8  2020-11-26T12:59Z        39                    12                     2   \n",
       "9  2020-11-26T12:59Z        60                     2                     2   \n",
       "\n",
       "                          station_name       station_geo  operative  \n",
       "0        Benjamin Godard - Victor Hugo  48.86598,2.27572       True  \n",
       "1   André Mazet - Saint-André des Arts  48.85376,2.33910       True  \n",
       "2  Charonne - Robert et Sonia Delauney  48.85591,2.39257       True  \n",
       "3                   Toudouze - Clauzel  48.87930,2.33736       True  \n",
       "4                      Mairie du 12ème  48.84086,2.38755       True  \n",
       "5                Harpe - Saint-Germain  48.85152,2.34367       True  \n",
       "6             Jourdan - Stade Charléty  48.81943,2.34334       True  \n",
       "7           Jouffroy d'Abbans - Wagram  48.88197,2.30113       True  \n",
       "8         Guersant - Gouvion-Saint-Cyr  48.88288,2.28767       True  \n",
       "9                   Alibert - Jemmapes  48.87104,2.36610       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données vacances :\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "holiday_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_date",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "end_date",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "zones",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "a63e8930-f11e-4aa8-94a1-d2d3750e0ed2",
       "rows": [
        [
         "0",
         "2030-01-01 00:00:00",
         "1er janvier",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "2030-04-22 00:00:00",
         "Lundi de Pâques",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "2030-05-01 00:00:00",
         "1er mai",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "2030-05-08 00:00:00",
         "8 mai",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "2030-05-30 00:00:00",
         "Ascension",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "2030-06-10 00:00:00",
         "Lundi de Pentecôte",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "2030-07-14 00:00:00",
         "14 juillet",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "2030-08-15 00:00:00",
         "Assomption",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "2030-11-01 00:00:00",
         "Toussaint",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "2030-11-11 00:00:00",
         "11 novembre",
         "holiday",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>zones</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030-01-01</td>\n",
       "      <td>1er janvier</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030-04-22</td>\n",
       "      <td>Lundi de Pâques</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2030-05-01</td>\n",
       "      <td>1er mai</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2030-05-08</td>\n",
       "      <td>8 mai</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030-05-30</td>\n",
       "      <td>Ascension</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2030-06-10</td>\n",
       "      <td>Lundi de Pentecôte</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2030-07-14</td>\n",
       "      <td>14 juillet</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2030-08-15</td>\n",
       "      <td>Assomption</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2030-11-01</td>\n",
       "      <td>Toussaint</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2030-11-11</td>\n",
       "      <td>11 novembre</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date        holiday_name     type start_date end_date zones  \\\n",
       "0 2030-01-01         1er janvier  holiday        NaT      NaT   NaN   \n",
       "1 2030-04-22     Lundi de Pâques  holiday        NaT      NaT   NaN   \n",
       "2 2030-05-01             1er mai  holiday        NaT      NaT   NaN   \n",
       "3 2030-05-08               8 mai  holiday        NaT      NaT   NaN   \n",
       "4 2030-05-30           Ascension  holiday        NaT      NaT   NaN   \n",
       "5 2030-06-10  Lundi de Pentecôte  holiday        NaT      NaT   NaN   \n",
       "6 2030-07-14          14 juillet  holiday        NaT      NaT   NaN   \n",
       "7 2030-08-15          Assomption  holiday        NaT      NaT   NaN   \n",
       "8 2030-11-01           Toussaint  holiday        NaT      NaT   NaN   \n",
       "9 2030-11-11         11 novembre  holiday        NaT      NaT   NaN   \n",
       "\n",
       "  description  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "5         NaN  \n",
       "6         NaN  \n",
       "7         NaN  \n",
       "8         NaN  \n",
       "9         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données météo :\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_id",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "station_name",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "lat_deg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon_deg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alt_m",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "precip_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_dur_min",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "temp_c",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dewpoint_c",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_rel_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_speed_10m_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_dir_deg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_gust_10m_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pressure_hpa",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloud_oktas",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "insolation_min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "global_radiation_j_cm2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wmo_present_weather",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "81ff0cfd-4903-435a-8415-183c314e74d7",
       "rows": [
        [
         "0",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 00:00:00",
         "0.0",
         "0",
         "0.7",
         "-3.3",
         "74.0",
         "8.9",
         "140.0",
         "14.0",
         "1013.8",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 01:00:00",
         "0.0",
         "0",
         "0.9",
         "-3.3",
         "74.0",
         "9.7",
         "150.0",
         "14.7",
         "1013.1",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 02:00:00",
         "0.0",
         "0",
         "0.7",
         "-2.2",
         "81.0",
         "10.2",
         "140.0",
         "14.3",
         "1012.6",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 03:00:00",
         "0.0",
         "20",
         "0.8",
         "-2.2",
         "81.0",
         "11.5",
         "130.0",
         "16.5",
         "1011.0",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 04:00:00",
         "0.0",
         "0",
         "0.9",
         "-2.0",
         "82.0",
         "12.9",
         "120.0",
         "18.0",
         "1008.7",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 05:00:00",
         "0.0",
         "10",
         "0.5",
         "-1.2",
         "89.0",
         "13.7",
         "120.0",
         "20.2",
         "1006.4",
         "8.0",
         "0.0",
         "0.0",
         "70.0"
        ],
        [
         "6",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 06:00:00",
         "0.0",
         "60",
         "0.6",
         "-0.5",
         "92.0",
         "14.0",
         "120.0",
         "22.3",
         "1005.0",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ],
        [
         "7",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 07:00:00",
         "0.4",
         "60",
         "0.6",
         "0.0",
         "96.0",
         "14.6",
         "120.0",
         "22.1",
         "1003.6",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ],
        [
         "8",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 08:00:00",
         "1.2",
         "60",
         "0.8",
         "0.2",
         "96.0",
         "15.0",
         "120.0",
         "23.6",
         "1001.7",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ],
        [
         "9",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 09:00:00",
         "5.1",
         "60",
         "0.9",
         "0.4",
         "97.0",
         "14.1",
         "110.0",
         "21.2",
         "1001.2",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat_deg</th>\n",
       "      <th>lon_deg</th>\n",
       "      <th>alt_m</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>precip_dur_min</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>dewpoint_c</th>\n",
       "      <th>humidity_rel_pct</th>\n",
       "      <th>wind_speed_10m_ms</th>\n",
       "      <th>wind_dir_deg</th>\n",
       "      <th>wind_gust_10m_ms</th>\n",
       "      <th>pressure_hpa</th>\n",
       "      <th>cloud_oktas</th>\n",
       "      <th>insolation_min</th>\n",
       "      <th>global_radiation_j_cm2</th>\n",
       "      <th>wmo_present_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>140.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1013.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>150.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>1012.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>120.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>89.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>1006.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>22.3</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 07:00:00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>60</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>120.0</td>\n",
       "      <td>22.1</td>\n",
       "      <td>1003.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 08:00:00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1001.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 09:00:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>1001.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id station_name    lat_deg    lon_deg  alt_m           timestamp  \\\n",
       "0   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 00:00:00   \n",
       "1   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 01:00:00   \n",
       "2   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 02:00:00   \n",
       "3   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 03:00:00   \n",
       "4   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 04:00:00   \n",
       "5   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 05:00:00   \n",
       "6   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 06:00:00   \n",
       "7   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 07:00:00   \n",
       "8   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 08:00:00   \n",
       "9   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 09:00:00   \n",
       "\n",
       "   precip_mm  precip_dur_min  temp_c  dewpoint_c  humidity_rel_pct  \\\n",
       "0        0.0               0     0.7        -3.3              74.0   \n",
       "1        0.0               0     0.9        -3.3              74.0   \n",
       "2        0.0               0     0.7        -2.2              81.0   \n",
       "3        0.0              20     0.8        -2.2              81.0   \n",
       "4        0.0               0     0.9        -2.0              82.0   \n",
       "5        0.0              10     0.5        -1.2              89.0   \n",
       "6        0.0              60     0.6        -0.5              92.0   \n",
       "7        0.4              60     0.6         0.0              96.0   \n",
       "8        1.2              60     0.8         0.2              96.0   \n",
       "9        5.1              60     0.9         0.4              97.0   \n",
       "\n",
       "   wind_speed_10m_ms  wind_dir_deg  wind_gust_10m_ms  pressure_hpa  \\\n",
       "0                8.9         140.0              14.0        1013.8   \n",
       "1                9.7         150.0              14.7        1013.1   \n",
       "2               10.2         140.0              14.3        1012.6   \n",
       "3               11.5         130.0              16.5        1011.0   \n",
       "4               12.9         120.0              18.0        1008.7   \n",
       "5               13.7         120.0              20.2        1006.4   \n",
       "6               14.0         120.0              22.3        1005.0   \n",
       "7               14.6         120.0              22.1        1003.6   \n",
       "8               15.0         120.0              23.6        1001.7   \n",
       "9               14.1         110.0              21.2        1001.2   \n",
       "\n",
       "   cloud_oktas  insolation_min  global_radiation_j_cm2  wmo_present_weather  \n",
       "0          8.0             0.0                     0.0                  0.0  \n",
       "1          8.0             0.0                     0.0                  0.0  \n",
       "2          8.0             0.0                     0.0                  0.0  \n",
       "3          8.0             0.0                     0.0                  0.0  \n",
       "4          8.0             0.0                     0.0                  0.0  \n",
       "5          8.0             0.0                     0.0                 70.0  \n",
       "6          8.0             0.0                     0.0                 68.0  \n",
       "7          8.0             0.0                     0.0                 68.0  \n",
       "8          8.0             0.0                     0.0                 68.0  \n",
       "9          8.0             0.0                     0.0                 68.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "velib_historical_dataframe = VelibCsvReader().read_dataframe()\n",
    "print(\"Données Vélos :\")\n",
    "display(velib_historical_dataframe.head(10))\n",
    "\n",
    "holidays_instance = HolidaysAPI()\n",
    "public_holidays_dataframe = holidays_instance.fetch_public_holidays()\n",
    "vacations_dataframe = holidays_instance.fetch_school_vacations()\n",
    "public_holidays_dataframe[\"type\"] = \"holiday\"\n",
    "vacations_dataframe[\"type\"] = \"vacation\"\n",
    "calendar_dataframe = pd.concat([public_holidays_dataframe, vacations_dataframe], ignore_index=True)\n",
    "print(\"Données vacances :\")\n",
    "display(calendar_dataframe.head(10))\n",
    "\n",
    "weather_dataframe = WeatherCsvReader().read_standardized()\n",
    "print(\"Données météo :\")\n",
    "display(weather_dataframe.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f7988",
   "metadata": {},
   "source": [
    "# Modélisation des données\n",
    "## Fonctions de modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c1bf5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Préparation des données...\n",
      "⚙️ Construction des features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_29840\\2493534093.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"fill_rate\"] = (df[\"available_total\"] / df[\"capacity\"].replace(0, pd.NA)).fillna(0).clip(0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 Seuils appliqués: target_empty >= 0.83 | target_full >= 0.17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureBuilder:\n",
    "    def __init__(self, velib, weather, calendar):\n",
    "        self.velib = velib\n",
    "        self.weather = weather\n",
    "        self.calendar = calendar\n",
    "\n",
    "    def _preprocess(self, velib, weather, calendar):\n",
    "        \"\"\"Prépare et fusionne les datasets (fusion horaire simple)\"\"\"\n",
    "        print(\"🧹 Préparation des données...\")\n",
    "\n",
    "        # Nettoyage des dates (UTC)\n",
    "        weather[\"timestamp\"] = pd.to_datetime(weather[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "        velib[\"time\"] = pd.to_datetime(velib[\"time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "        # Clé horaire\n",
    "        weather[\"ts_hour\"] = weather[\"timestamp\"].dt.floor(\"h\")\n",
    "        velib[\"ts_hour\"] = velib[\"time\"].dt.floor(\"h\")\n",
    "\n",
    "        # ===== Flags météo très simples (0/1) =====\n",
    "        precip = pd.to_numeric(weather.get(\"precip_mm\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        precip_dur = pd.to_numeric(weather.get(\"precip_dur_min\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        ws = pd.to_numeric(weather.get(\"wind_speed_10m_ms\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        wg = pd.to_numeric(weather.get(\"wind_gust_10m_ms\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        cloud_oktas = pd.to_numeric(weather.get(\"cloud_oktas\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        # Règles binaires\n",
    "        weather[\"pluie\"]  = ((precip >= 0.1) | (precip_dur >= 5)).astype(int)\n",
    "        weather[\"vent\"]   = ((ws >= 8.0) | (wg >= 10.8)).astype(int)\n",
    "        weather[\"soleil\"] = cloud_oktas.le(2).fillna(False).astype(int)\n",
    "        weather[\"nuage\"]  = cloud_oktas.ge(6).fillna(False).astype(int)\n",
    "\n",
    "        # 1 ligne par heure\n",
    "        weather_flags = (\n",
    "            weather.sort_values(\"timestamp\")\n",
    "                   .drop_duplicates(subset=[\"ts_hour\"], keep=\"last\")\n",
    "                   [[\"ts_hour\", \"pluie\", \"vent\", \"soleil\", \"nuage\"]]\n",
    "                   .copy()\n",
    "        )\n",
    "        weather_flags[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]] = weather_flags[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]].fillna(0).astype(int)\n",
    "\n",
    "        # Fusion météo ↔ Vélib\n",
    "        df = velib.merge(weather_flags, on=\"ts_hour\", how=\"left\")\n",
    "        for c in [\"pluie\", \"vent\", \"soleil\", \"nuage\"]:\n",
    "            if c not in df.columns:\n",
    "                df[c] = 0\n",
    "        df[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]] = df[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]].fillna(0).astype(int)\n",
    "\n",
    "        # Ajout calendrier\n",
    "        df[\"date\"] = df[\"time\"].dt.tz_convert(\"Europe/Paris\").dt.date\n",
    "        cal = calendar.copy()\n",
    "        cal[\"date\"] = pd.to_datetime(cal[\"start_date\"]).dt.date\n",
    "        cal[\"holiday_flag\"] = 1\n",
    "        df = df.merge(cal[[\"date\", \"holiday_flag\"]].drop_duplicates(), on=\"date\", how=\"left\")\n",
    "        df[\"holiday_flag\"] = df[\"holiday_flag\"].fillna(0).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _feature_engineering(self, df):\n",
    "        \"\"\"Crée des variables dérivées utiles pour la prédiction\"\"\"\n",
    "        print(\"⚙️ Construction des features...\")\n",
    "\n",
    "        # TOTAL vélos dispo & bornes libres\n",
    "        df[\"available_total\"] = df[\"available_mechanical\"].fillna(0) + df[\"available_electrical\"].fillna(0)\n",
    "        df[\"docks_available\"] = df[\"capacity\"].fillna(0) - df[\"available_total\"]\n",
    "\n",
    "        # Taux d'occupation\n",
    "        df[\"fill_rate\"] = (df[\"available_total\"] / df[\"capacity\"].replace(0, pd.NA)).fillna(0).clip(0, 1)\n",
    "\n",
    "        # ✅ Calcul des ratios sûrs\n",
    "        ratio_empty = (df[\"docks_available\"] / df[\"capacity\"].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0).clip(0, 1)\n",
    "        ratio_full = (df[\"available_total\"] / df[\"capacity\"].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0).clip(0, 1)\n",
    "\n",
    "        # ✅ Définition automatique des seuils (adaptés à ta distribution)\n",
    "        empty_threshold = ratio_empty.quantile(0.70)  # stations avec beaucoup de place\n",
    "        full_threshold  = ratio_full.quantile(0.30)   # stations avec assez de vélos\n",
    "\n",
    "        print(f\"📏 Seuils appliqués: target_empty >= {empty_threshold:.2f} | target_full >= {full_threshold:.2f}\")\n",
    "\n",
    "        # ✅ Cibles binaires équilibrées\n",
    "        df[\"target_empty\"] = (ratio_empty >= empty_threshold).astype(int)\n",
    "        df[\"target_full\"]  = (ratio_full >= full_threshold).astype(int)\n",
    "\n",
    "        # Heures / jours\n",
    "        df = df.sort_values([\"station_name\", \"time\"])\n",
    "        df[\"hour\"] = df[\"time\"].dt.hour\n",
    "        df[\"day_of_week\"] = df[\"time\"].dt.day_name()\n",
    "\n",
    "        # Moyenne glissante (3h) par station\n",
    "        df[\"rolling_fill_rate\"] = (\n",
    "            df.groupby(\"station_name\")[\"fill_rate\"].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "        )\n",
    "\n",
    "        # Week-end\n",
    "        df[\"is_weekend\"] = df[\"day_of_week\"].isin([\"Saturday\", \"Sunday\"]).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Exécution complète du pipeline de feature engineering\"\"\"\n",
    "        merged = self._preprocess(self.velib, self.weather, self.calendar)\n",
    "        if merged is None:\n",
    "            raise RuntimeError(\"[FeatureBuilder.run] _preprocess a renvoyé None (attendu: DataFrame).\")\n",
    "        features = self._feature_engineering(merged)\n",
    "        return features\n",
    "\n",
    "\n",
    "# 🧪 Exemple d'exécution\n",
    "feature_dataframe = FeatureBuilder(\n",
    "    velib_historical_dataframe,\n",
    "    weather_dataframe,\n",
    "    calendar_dataframe\n",
    ").run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56712983",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d0d6a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Distribution finale de target_empty:\n",
      "                count  percent\n",
      "target_empty                  \n",
      "0             7672453    69.83\n",
      "1             3314277    30.17\n",
      "\n",
      "📊 Distribution finale de target_full:\n",
      "               count  percent\n",
      "target_full                  \n",
      "0            3252063     29.6\n",
      "1            7734667     70.4\n",
      "\n",
      "📈 Corrélation entre les deux cibles:\n",
      "              target_empty  target_full\n",
      "target_empty      1.000000    -0.958444\n",
      "target_full      -0.958444     1.000000\n"
     ]
    }
   ],
   "source": [
    "for col in [\"target_empty\", \"target_full\"]:\n",
    "    counts = feature_dataframe[col].value_counts().sort_index()\n",
    "    percents = feature_dataframe[col].value_counts(normalize=True).sort_index() * 100\n",
    "    print(f\"\\n📊 Distribution finale de {col}:\")\n",
    "    print(pd.DataFrame({\"count\": counts, \"percent\": percents.round(2)}))\n",
    "\n",
    "print(\"\\n📈 Corrélation entre les deux cibles:\")\n",
    "print(feature_dataframe[[\"target_empty\", \"target_full\"]].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72dbbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import joblib\n",
    "\n",
    "\n",
    "class VelibSimpleModel:\n",
    "    \"\"\"\n",
    "    Modèle simple et lisible pour prédire si une station sera vide/pleine.\n",
    "\n",
    "    Principes :\n",
    "    - Utilise uniquement les colonnes déjà présentes dans votre DataFrame d'exemple\n",
    "      (pas de météo, pas de mapping).\n",
    "    - Encodage temporel cyclique (à partir de 'time').\n",
    "    - Imputation légère + standardisation pour les numériques, OHE pour station_id.\n",
    "    - API courte: fit / predict_proba / predict_label / save / load\n",
    "    - Paramètres regroupés en dict (to_config / from_config).\n",
    "    \"\"\"\n",
    "\n",
    "    # Colonnes de base attendues (selon votre DataFrame d'entrée)\n",
    "    BASE_NUM_SCALED: List[str] = [\n",
    "        #\"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "        # features temporelles\n",
    "        \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\",\n",
    "    ]\n",
    "    BASE_BIN_PASSTHROUGH: List[str] = [\n",
    "        \"holiday_flag\", \"is_weekend\", \"operative\",\n",
    "        \"pluie\", \"vent\", \"soleil\", \"nuage\",\n",
    "    ]\n",
    "    BASE_CATEG: List[str] = [\"station_name\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str = \"target_empty\",        # ou \"target_full\"\n",
    "        model_type: str = \"gb\",                  # \"gb\" ou \"logit\"\n",
    "        timezone: str = \"Europe/Paris\",\n",
    "        random_state: int = 42,\n",
    "        test_size: float = 0.2,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        print(\"[__init__] → Début initialisation du modèle\")\n",
    "        self.target_col = target_col\n",
    "        self.model_type = model_type\n",
    "        self.timezone = timezone\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Objets entraînés\n",
    "        self.feature_list_: List[str] = []\n",
    "        self.preprocessor_: Optional[ColumnTransformer] = None\n",
    "        self.pipeline_: Optional[Pipeline] = None\n",
    "        print(\"[__init__] ✓ Fin initialisation du modèle\")\n",
    "\n",
    "    # ------------- Helpers ---------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "        print(\"[_ensure_columns] → Vérification des colonnes requises...\")\n",
    "        missing = [c for c in cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "        print(\"[_ensure_columns] ✓ Toutes les colonnes sont présentes.\")\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute hour/dow/month + encodage cyclique à partir de la colonne 'time'.\n",
    "        On ne dépend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "        \"\"\"\n",
    "        print(\"[_add_time_features] → Début génération des features temporelles...\")\n",
    "        if \"time\" not in df.columns:\n",
    "            raise ValueError(\"La colonne 'time' est requise.\")\n",
    "        ts = pd.to_datetime(df[\"time\"], utc=True).dt.tz_convert(self.timezone)\n",
    "\n",
    "        out = df.copy()\n",
    "        out[\"hour\"] = ts.dt.hour\n",
    "        out[\"dow\"] = ts.dt.weekday     # 0 = lundi\n",
    "        out[\"month\"] = ts.dt.month\n",
    "\n",
    "        out[\"hour_ssin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "        out[\"hour_ccos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "        out[\"dow_sin\"]   = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "        out[\"dow_cos\"]   = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "        print(\"[_add_time_features] ✓ Features temporelles ajoutées.\")\n",
    "        return out\n",
    "\n",
    "    def _build_preprocessor(self) -> None:\n",
    "        \"\"\"\n",
    "        Préprocesseur:\n",
    "        - Numériques (imputation médiane + standardisation)\n",
    "        - Binaires (imputation la plus fréquente, pas de scaling)\n",
    "        - Catégorielles (OHE handle_unknown='ignore')\n",
    "        \"\"\"\n",
    "        print(\"[_build_preprocessor] → Construction du préprocesseur...\")\n",
    "        # Gestion valeurs manquantes + standardisation\n",
    "        num_pipe = Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\", StandardScaler()),\n",
    "        ])\n",
    "        # Gestion valeurs manquantes (pas scaler pour garder 0/1 lisible)\n",
    "        bin_pipe = Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        ])\n",
    "        # Transforme variables catégorielles en binaire\n",
    "        cat_pipe = Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "            (\"ohe\", OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                dtype=np.float32,\n",
    "                sparse_output=True,\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Liste des features finales\n",
    "        self.feature_list_ = (\n",
    "            self.BASE_NUM_SCALED\n",
    "            + self.BASE_BIN_PASSTHROUGH\n",
    "            + self.BASE_CATEG\n",
    "        )\n",
    "\n",
    "        # Applique les 3 pipelines de transformation + drop autres colonnes\n",
    "        self.preprocessor_ = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_pipe, self.BASE_NUM_SCALED),\n",
    "                (\"bin\", bin_pipe, self.BASE_BIN_PASSTHROUGH),\n",
    "                (\"cat\", cat_pipe, self.BASE_CATEG),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            sparse_threshold=1.0,\n",
    "        )\n",
    "        print(\"[_build_preprocessor] ✓ Préprocesseur construit.\")\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, df: pd.DataFrame, with_target: bool\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        - Ajoute les features temporelles\n",
    "        - Vérifie la présence des colonnes attendues\n",
    "        - Renvoie X (et y si with_target)\n",
    "        \"\"\"\n",
    "        print(\"[_prepare_features] → Préparation des features...\")\n",
    "        # Ajoute les features temporelles\n",
    "        df2 = self._add_time_features(df)\n",
    "\n",
    "        # Construire le préprocesseur si pas encore fait\n",
    "        if self.preprocessor_ is None:\n",
    "            self._build_preprocessor()\n",
    "\n",
    "        # Vérifier les colonnes d'entrée attendues\n",
    "        needed = (\n",
    "            set(self.BASE_NUM_SCALED + self.BASE_BIN_PASSTHROUGH + self.BASE_CATEG)\n",
    "            - {\"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"}  # créées ici\n",
    "        )\n",
    "        self._ensure_columns(df2, sorted(needed))\n",
    "\n",
    "        X = df2[self.feature_list_] # Liste de colonne créée dans _build_preprocessor()\n",
    "\n",
    "        y = None\n",
    "        if with_target: # Vérification de la présence de la colonne cible (à prédire)\n",
    "            if self.target_col not in df2.columns:\n",
    "                raise ValueError(f\"Colonne cible manquante: '{self.target_col}'\")\n",
    "            y = df2[self.target_col].astype(int)\n",
    "\n",
    "        # Renvoi les colonnes de paramètre ainsi que la colonne cible \n",
    "        print(\"[_prepare_features] ✓ Features préparées.\")\n",
    "        return X, y\n",
    "\n",
    "    # ------------- API publique ---------------\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Entraîne le modèle choisi et renvoie des métriques simples (AUC).\n",
    "        \"\"\"\n",
    "        print(\"[fit] → Début entraînement du modèle...\")\n",
    "        \n",
    "        self._build_preprocessor()# (ré)initialise un préprocesseur propre\n",
    "        X, y = self._prepare_features(df, with_target=True) # Préparation des features\n",
    "\n",
    "        # Choix du classifieur\n",
    "        if self.model_type == \"logit\":\n",
    "            classifier = LogisticRegression(max_iter=300, solver=\"saga\", verbose=1, random_state=self.random_state)\n",
    "        else:\n",
    "            classifier = GradientBoostingClassifier(random_state=self.random_state, verbose=1)\n",
    "\n",
    "        # Assemblage préprocesseur + classifieur\n",
    "        self.pipeline_ = Pipeline(steps=[(\"preprocessor\", self.preprocessor_), (\"classifier\", classifier)])\n",
    "\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.test_size,\n",
    "            stratify=y,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "        print(\"[fit] → Entraînement en cours...\")\n",
    "        self.pipeline_.fit(X_tr, y_tr)\n",
    "\n",
    "        print(\"[fit] ✓ Entraînement terminé. Évaluation en cours...\")\n",
    "\n",
    "        proba = self.pipeline_.predict_proba(X_va)[:, 1]\n",
    "        y_pred = (proba >= 0.5).astype(int)  # seuil simple\n",
    "\n",
    "        acc = accuracy_score(y_va, y_pred)\n",
    "        f1  = f1_score(y_va, y_pred)\n",
    "\n",
    "        metrics = {\n",
    "            \"val_accuracy\": float(acc),\n",
    "            \"val_f1\": float(f1),\n",
    "            \"n_samples_train\": int(len(X_tr)),\n",
    "            \"n_samples_val\": int(len(X_va)),\n",
    "        }\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[{self.target_col}] {self.model_type.upper()}  \"\n",
    "                f\"ACC={metrics['val_accuracy']:.3f}  F1={metrics['val_f1']:.3f}  \"\n",
    "                f\"(train={metrics['n_samples_train']}, val={metrics['n_samples_val']})\")\n",
    "\n",
    "\n",
    "        print(\"[fit] ✓ Fin de l'entraînement et des métriques.\")\n",
    "        return metrics\n",
    "\n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Probabilité d'être positif (ex: vide si target_empty).\"\"\"\n",
    "        print(\"[predict_proba] → Début prédiction des probabilités...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        # NE PAS reconstruire le préprocesseur ici\n",
    "        X, _ = self._prepare_features(df, with_target=False)\n",
    "        print(\"[predict_proba] ✓ Fin prédiction des probabilités.\")\n",
    "        return self.pipeline_.predict_proba(X)[:, 1]\n",
    "\n",
    "    def predict_label(self, df: pd.DataFrame, threshold: float = 0.5) -> pd.Series:\n",
    "        \"\"\"Label binaire selon un seuil.\"\"\"\n",
    "        print(\"[predict_label] → Début prédiction des labels...\")\n",
    "        p = self.predict_proba(df)\n",
    "        print(\"[predict_label] ✓ Fin prédiction des labels.\")\n",
    "        return pd.Series((p >= threshold).astype(int), index=df.index, name=f\"{self.target_col}_pred\")\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Sauvegarde le pipeline complet (prétraitement + modèle).\"\"\"\n",
    "        print(\"[save] → Sauvegarde du modèle...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        joblib.dump(self.pipeline_, path)\n",
    "        print(\"[save] ✓ Modèle sauvegardé.\")\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Charge un pipeline entraîné (préprocesseur inclus).\"\"\"\n",
    "        print(\"[load] → Chargement du modèle...\")\n",
    "        self.pipeline_ = joblib.load(path)\n",
    "        # on récupère le préprocesseur et la liste de features du pipeline sauvegardé\n",
    "        if hasattr(self.pipeline_, \"named_steps\") and \"preprocessor\" in self.pipeline_.named_steps:\n",
    "            self.preprocessor_ = self.pipeline_.named_steps[\"preprocessor\"]\n",
    "        # La feature_list_ est utile seulement pour _prepare_features (ordre des colonnes en entrée)\n",
    "        # On la reconstruit à partir des attributs de classe pour rester déterministe :\n",
    "        self.feature_list_ = (\n",
    "            self.BASE_NUM_SCALED + self.BASE_BIN_PASSTHROUGH + self.BASE_CATEG\n",
    "        )\n",
    "        print(\"[load] ✓ Modèle chargé avec succès.\")\n",
    "\n",
    "    # --------- Config dict ---------\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg: Dict) -> \"VelibSimpleModel\":\n",
    "        print(\"[from_config] → Création du modèle depuis un dictionnaire de config...\")\n",
    "        model = cls(**cfg)\n",
    "        print(\"[from_config] ✓ Modèle créé depuis la config.\")\n",
    "        return model\n",
    "\n",
    "    def to_config(self) -> Dict:\n",
    "        print(\"[to_config] → Export de la configuration du modèle...\")\n",
    "        cfg = {\n",
    "            \"target_col\": self.target_col,\n",
    "            \"model_type\": self.model_type,\n",
    "            \"timezone\": self.timezone,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"test_size\": self.test_size,\n",
    "            \"verbose\": self.verbose,\n",
    "        }\n",
    "        print(\"[to_config] ✓ Configuration exportée.\")\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cb6b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[from_config] → Création du modèle depuis un dictionnaire de config...\n",
      "[__init__] → Début initialisation du modèle\n",
      "[__init__] ✓ Fin initialisation du modèle\n",
      "[from_config] ✓ Modèle créé depuis la config.\n",
      "[fit] → Début entraînement du modèle...\n",
      "[_build_preprocessor] → Construction du préprocesseur...\n",
      "[_build_preprocessor] ✓ Préprocesseur construit.\n",
      "[_prepare_features] → Préparation des features...\n",
      "[_add_time_features] → Début génération des features temporelles...\n",
      "[_add_time_features] ✓ Features temporelles ajoutées.\n",
      "[_ensure_columns] → Vérification des colonnes requises...\n",
      "[_ensure_columns] ✓ Toutes les colonnes sont présentes.\n",
      "[_prepare_features] ✓ Features préparées.\n",
      "[fit] → Entraînement en cours...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2182           31.98m\n",
      "         2           1.2131           31.16m\n",
      "         3           1.2089           31.14m\n",
      "         4           1.2055           30.70m\n",
      "         5           1.2026           30.21m\n",
      "         6           1.2002           29.90m\n",
      "         7           1.1980           29.66m\n",
      "         8           1.1963           29.94m\n",
      "         9           1.1948           29.41m\n",
      "        10           1.1933           28.88m\n",
      "        20           1.1860           25.36m\n",
      "        30           1.1811           21.99m\n",
      "        40           1.1770           18.77m\n",
      "        50           1.1730           15.24m\n",
      "        60           1.1697           11.99m\n",
      "        70           1.1665            8.87m\n",
      "        80           1.1635            5.86m\n",
      "        90           1.1607            2.91m\n",
      "       100           1.1579            0.00s\n",
      "[fit] ✓ Entraînement terminé. Évaluation en cours...\n",
      "[target_empty] GB  ACC=0.718  F1=0.130  (train=8789384, val=2197346)\n",
      "[fit] ✓ Fin de l'entraînement et des métriques.\n",
      "📊 Résultats de l'entraînement :\n",
      "  - val_accuracy: 0.7178127613948827\n",
      "  - val_f1: 0.12989123342225314\n",
      "  - n_samples_train: 8789384\n",
      "  - n_samples_val: 2197346\n",
      "[save] → Sauvegarde du modèle...\n",
      "[save] ✓ Modèle sauvegardé.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charger vos données\n",
    "df = feature_dataframe # pd.read_csv(\"velib_data.csv\")  # ou df = votre_dataframe déjà chargé\n",
    "\n",
    "# 2. Créer le modèle avec vos paramètres\n",
    "config = {\n",
    "    \"target_col\": \"target_empty\",     # ou \"target_full\"\n",
    "    \"model_type\": \"gb\",               # \"gb\" ou \"logit\"\n",
    "    \"timezone\": \"Europe/Paris\",\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.2,\n",
    "    \"verbose\": True\n",
    "}\n",
    "model = VelibSimpleModel.from_config(config)\n",
    "\n",
    "# 3. Lancer l'entraînement\n",
    "metrics = model.fit(feature_dataframe)\n",
    "\n",
    "# 4. Afficher les résultats\n",
    "print(\"📊 Résultats de l'entraînement :\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "# 5. (Optionnel) Sauvegarder le modèle\n",
    "model.save(\"velib_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6902c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_velib.py\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# --- Helpers communs ---------------------------------------------------------\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "    print(\"[_ensure_columns] → Vérification des colonnes requises...\")\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "    print(\"[_ensure_columns] ✓ Toutes les colonnes sont présentes.\")\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, timezone: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute hour/dow/month + encodage cyclique à partir de la colonne 'time'.\n",
    "    On ne dépend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "    \"\"\"\n",
    "    print(\"[_add_time_features] → Début génération des features temporelles...\")\n",
    "    if \"time\" not in df.columns:\n",
    "        raise ValueError(\"La colonne 'time' est requise.\")\n",
    "    ts = pd.to_datetime(df[\"time\"], utc=True).dt.tz_convert(timezone)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = ts.dt.hour\n",
    "    out[\"dow\"] = ts.dt.weekday     # 0 = lundi\n",
    "    out[\"month\"] = ts.dt.month\n",
    "\n",
    "    out[\"hour_ssin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_ccos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"dow_sin\"]   = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"]   = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "    print(\"[_add_time_features] ✓ Features temporelles ajoutées.\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_preprocessor(\n",
    "    BASE_NUM_SCALED: List[str],\n",
    "    BASE_BIN_PASSTHROUGH: List[str],\n",
    "    BASE_CATEG: List[str],\n",
    ") -> Tuple[ColumnTransformer, List[str]]:\n",
    "    \"\"\"\n",
    "    Préprocesseur:\n",
    "    - Numériques (imputation médiane + standardisation)\n",
    "    - Binaires (imputation la plus fréquente, pas de scaling)\n",
    "    - Catégorielles (OHE handle_unknown='ignore')\n",
    "    \"\"\"\n",
    "    print(\"[_build_preprocessor] → Construction du préprocesseur...\")\n",
    "    # Gestion valeurs manquantes + standardisation\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\", StandardScaler()),\n",
    "    ])\n",
    "    # Gestion valeurs manquantes (pas scaler pour garder 0/1 lisible)\n",
    "    bin_pipe = Pipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ])\n",
    "    # Transforme variables catégorielles en binaire\n",
    "    cat_pipe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "    # Liste des features finales\n",
    "    feature_list = BASE_NUM_SCALED + BASE_BIN_PASSTHROUGH + BASE_CATEG\n",
    "\n",
    "    # Applique les 3 pipelines de transformation + drop autres colonnes\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, BASE_NUM_SCALED),\n",
    "            (\"bin\", bin_pipe, BASE_BIN_PASSTHROUGH),\n",
    "            (\"cat\", cat_pipe, BASE_CATEG),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    print(\"[_build_preprocessor] ✓ Préprocesseur construit.\")\n",
    "    return preprocessor, feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94856e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trainer_and_predictor.py\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import joblib\n",
    "\n",
    "# <<< NEW: utils communs >>>\n",
    "# from utils_velib import ensure_columns as _utils_ensure_columns\n",
    "# from utils_velib import add_time_features as _utils_add_time_features\n",
    "# from utils_velib import build_preprocessor as _utils_build_preprocessor\n",
    "\n",
    "\n",
    "# ============================\n",
    "#   ENTRAÎNEMENT : ModelTrainer\n",
    "# ============================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Modèle simple et lisible pour prédire si une station sera vide/pleine.\n",
    "\n",
    "    Principes :\n",
    "    - Utilise uniquement les colonnes déjà présentes dans votre DataFrame d'exemple\n",
    "      (pas de météo, pas de mapping).\n",
    "    - Encodage temporel cyclique (à partir de 'time').\n",
    "    - Imputation légère + standardisation pour les numériques, OHE pour station_id.\n",
    "    - API courte: fit / predict_proba / predict_label / save / load\n",
    "    - Paramètres regroupés en dict (to_config / from_config).\n",
    "    \"\"\"\n",
    "\n",
    "    # Colonnes de base attendues (selon votre DataFrame d'entrée)\n",
    "    BASE_NUM_SCALED: List[str] = [\n",
    "        \"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "        # features temporelles\n",
    "        \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\",\n",
    "    ]\n",
    "    BASE_BIN_PASSTHROUGH: List[str] = [\"holiday_flag\", \"is_weekend\", \"operative\"]\n",
    "    BASE_CATEG: List[str] = [\"station_id\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str = \"target_empty\",        # ou \"target_full\"\n",
    "        model_type: str = \"gb\",                  # \"gb\" ou \"logit\"\n",
    "        timezone: str = \"Europe/Paris\",\n",
    "        random_state: int = 42,\n",
    "        test_size: float = 0.2,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        print(\"[__init__] → Début initialisation du modèle\")\n",
    "        self.target_col = target_col\n",
    "        self.model_type = model_type\n",
    "        self.timezone = timezone\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Objets entraînés\n",
    "        self.feature_list_: List[str] = []\n",
    "        self.preprocessor_: Optional[ColumnTransformer] = None\n",
    "        self.pipeline_: Optional[Pipeline] = None\n",
    "        print(\"[__init__] ✓ Fin initialisation du modèle\")\n",
    "\n",
    "    # ------------- Helpers ---------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "        print(\"[_ensure_columns] → Vérification des colonnes requises...\")\n",
    "        ensure_columns(df, cols)\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute hour/dow/month + encodage cyclique à partir de la colonne 'time'.\n",
    "        On ne dépend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "        \"\"\"\n",
    "        return add_time_features(df, self.timezone)\n",
    "\n",
    "    def _build_preprocessor(self) -> None:\n",
    "        \"\"\"\n",
    "        Préprocesseur:\n",
    "        - Numériques (imputation médiane + standardisation)\n",
    "        - Binaires (imputation la plus fréquente, pas de scaling)\n",
    "        - Catégorielles (OHE handle_unknown='ignore')\n",
    "        \"\"\"\n",
    "        self.preprocessor_, self.feature_list_ = build_preprocessor(\n",
    "            self.BASE_NUM_SCALED, self.BASE_BIN_PASSTHROUGH, self.BASE_CATEG\n",
    "        )\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, df: pd.DataFrame, with_target: bool\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        - Ajoute les features temporelles\n",
    "        - Vérifie la présence des colonnes attendues\n",
    "        - Renvoie X (et y si with_target)\n",
    "        \"\"\"\n",
    "        print(\"[_prepare_features] → Préparation des features...\")\n",
    "        # Ajoute les features temporelles\n",
    "        df2 = self._add_time_features(df)\n",
    "\n",
    "        # Construire le préprocesseur si pas encore fait\n",
    "        if self.preprocessor_ is None:\n",
    "            self._build_preprocessor()\n",
    "\n",
    "        # Vérifier les colonnes d'entrée attendues\n",
    "        needed = (\n",
    "            set(self.BASE_NUM_SCALED + self.BASE_BIN_PASSTHROUGH + self.BASE_CATEG)\n",
    "            - {\"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"}  # créées ici\n",
    "        )\n",
    "        self._ensure_columns(df2, sorted(needed))\n",
    "\n",
    "        X = df2[self.feature_list_] # Liste de colonne créée dans _build_preprocessor()\n",
    "\n",
    "        y = None\n",
    "        if with_target: # Vérification de la présence de la colonne cible (à prédire)\n",
    "            if self.target_col not in df2.columns:\n",
    "                raise ValueError(f\"Colonne cible manquante: '{self.target_col}'\")\n",
    "            y = df2[self.target_col].astype(int)\n",
    "\n",
    "        # Renvoi les colonnes de paramètre ainsi que la colonne cible \n",
    "        print(\"[_prepare_features] ✓ Features préparées.\")\n",
    "        return X, y\n",
    "\n",
    "    # ------------- API publique ---------------\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Entraîne le modèle choisi et renvoie des métriques simples (AUC).\n",
    "        \"\"\"\n",
    "        print(\"[fit] → Début entraînement du modèle...\")\n",
    "        \n",
    "        self._build_preprocessor()# (ré)initialise un préprocesseur propre\n",
    "        X, y = self._prepare_features(df, with_target=True) # Préparation des features\n",
    "\n",
    "        # Choix du classifieur\n",
    "        if self.model_type == \"logit\":\n",
    "            classifier = LogisticRegression(max_iter=300, solver=\"saga\", verbose=1, random_state=self.random_state)\n",
    "        else:\n",
    "            classifier = GradientBoostingClassifier(random_state=self.random_state, verbose=1)\n",
    "\n",
    "        # Assemblage préprocesseur + classifieur\n",
    "        self.pipeline_ = Pipeline(steps=[(\"preprocessor\", self.preprocessor_), (\"classifier\", classifier)])\n",
    "\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.test_size,\n",
    "            stratify=y,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "        print(\"[fit] → Entraînement en cours...\")\n",
    "        self.pipeline_.fit(X_tr, y_tr)\n",
    "        print(\"[fit] ✓ Entraînement terminé. Évaluation en cours...\")\n",
    "\n",
    "        proba = self.pipeline_.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_va, proba)\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "\n",
    "        metrics = {\n",
    "            \"val_auc\": float(auc),\n",
    "            \"n_samples_train\": int(len(X_tr)),\n",
    "            \"n_samples_val\": int(len(X_va)),\n",
    "        }\n",
    "        if self.verbose:\n",
    "            print(f\"[{self.target_col}] {self.model_type.upper()}  \"\n",
    "                  f\"AUC={metrics['val_auc']:.3f}  \"\n",
    "                  f\"(train={metrics['n_samples_train']}, val={metrics['n_samples_val']})\")\n",
    "\n",
    "        print(\"[fit] ✓ Fin de l'entraînement et des métriques.\")\n",
    "        return metrics\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Sauvegarde le pipeline complet (prétraitement + modèle).\"\"\"\n",
    "        print(\"[save] → Sauvegarde du modèle...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        joblib.dump(self.pipeline_, path)\n",
    "        print(\"[save] ✓ Modèle sauvegardé.\")\n",
    "\n",
    "    # --------- Config dict ---------\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg: Dict) -> \"ModelTrainer\":\n",
    "        print(\"[from_config] → Création du modèle depuis un dictionnaire de config...\")\n",
    "        model = cls(**cfg)\n",
    "        print(\"[from_config] ✓ Modèle créé depuis la config.\")\n",
    "        return model\n",
    "\n",
    "    def to_config(self) -> Dict:\n",
    "        print(\"[to_config] → Export de la configuration du modèle...\")\n",
    "        cfg = {\n",
    "            \"target_col\": self.target_col,\n",
    "            \"model_type\": self.model_type,\n",
    "            \"timezone\": self.timezone,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"test_size\": self.test_size,\n",
    "            \"verbose\": self.verbose,\n",
    "        }\n",
    "        print(\"[to_config] ✓ Configuration exportée.\")\n",
    "        return cfg\n",
    "\n",
    "\n",
    "# ============================\n",
    "#   PRÉDICTION : Predictor\n",
    "# ============================\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Prédicteur basé sur un pipeline entraîné.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timezone: str = \"Europe/Paris\", target_col: str = \"target_empty\", verbose: bool = True):\n",
    "        print(\"[Predictor.__init__] → Début initialisation du prédicteur\")\n",
    "        self.timezone = timezone\n",
    "        self.target_col = target_col\n",
    "        self.verbose = verbose\n",
    "        self.feature_list_: List[str] = []\n",
    "        self.preprocessor_: Optional[ColumnTransformer] = None\n",
    "        self.pipeline_: Optional[Pipeline] = None\n",
    "        print(\"[Predictor.__init__] ✓ Fin initialisation du prédicteur\")\n",
    "\n",
    "    # ------------- Helpers ---------------\n",
    "    # (fonctions conservées telles quelles pour préparer X comme à l'entraînement)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "        print(\"[_ensure_columns] → Vérification des colonnes requises...\")\n",
    "        ensure_columns(df, cols)\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute hour/dow/month + encodage cyclique à partir de la colonne 'time'.\n",
    "        On ne dépend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "        \"\"\"\n",
    "        return add_time_features(df, self.timezone)\n",
    "\n",
    "    def _build_preprocessor(self) -> None:\n",
    "        \"\"\"\n",
    "        Préprocesseur:\n",
    "        - Numériques (imputation médiane + standardisation)\n",
    "        - Binaires (imputation la plus fréquente, pas de scaling)\n",
    "        - Catégorielles (OHE handle_unknown='ignore')\n",
    "        \"\"\"\n",
    "        # Remarque : on reprend les mêmes noms que côté entraînement\n",
    "        BASE_NUM_SCALED = [\n",
    "            \"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "            \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\",\n",
    "        ]\n",
    "        BASE_BIN_PASSTHROUGH = [\"holiday_flag\", \"is_weekend\", \"operative\"]\n",
    "        BASE_CATEG = [\"station_id\"]\n",
    "\n",
    "        self.preprocessor_, self.feature_list_ = build_preprocessor(\n",
    "            BASE_NUM_SCALED, BASE_BIN_PASSTHROUGH, BASE_CATEG\n",
    "        )\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, df: pd.DataFrame, with_target: bool\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        - Ajoute les features temporelles\n",
    "        - Vérifie la présence des colonnes attendues\n",
    "        - Renvoie X (et y si with_target)\n",
    "        \"\"\"\n",
    "        print(\"[_prepare_features] → Préparation des features...\")\n",
    "        # Ajoute les features temporelles\n",
    "        df2 = self._add_time_features(df)\n",
    "\n",
    "        # Construire le préprocesseur si pas encore fait\n",
    "        if self.preprocessor_ is None:\n",
    "            self._build_preprocessor()\n",
    "\n",
    "        # Vérifier les colonnes d'entrée attendues\n",
    "        needed = (\n",
    "            set(self.feature_list_)  # mêmes colonnes qu'à l'entraînement\n",
    "            - {\"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"}  # créées ici\n",
    "        )\n",
    "        self._ensure_columns(df2, sorted(needed))\n",
    "\n",
    "        X = df2[self.feature_list_] # Liste de colonne créée dans _build_preprocessor()\n",
    "\n",
    "        y = None\n",
    "        if with_target: # Vérification de la présence de la colonne cible (à prédire)\n",
    "            if self.target_col not in df2.columns:\n",
    "                raise ValueError(f\"Colonne cible manquante: '{self.target_col}'\")\n",
    "            y = df2[self.target_col].astype(int)\n",
    "\n",
    "        # Renvoi les colonnes de paramètre ainsi que la colonne cible \n",
    "        print(\"[_prepare_features] ✓ Features préparées.\")\n",
    "        return X, y\n",
    "\n",
    "    # ------------- API publique ---------------\n",
    "\n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Probabilité d'être positif (ex: vide si target_empty).\"\"\"\n",
    "        print(\"[predict_proba] → Début prédiction des probabilités...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        # NE PAS reconstruire le préprocesseur ici\n",
    "        X, _ = self._prepare_features(df, with_target=False)\n",
    "        print(\"[predict_proba] ✓ Fin prédiction des probabilités.\")\n",
    "        return self.pipeline_.predict_proba(X)[:, 1]\n",
    "\n",
    "    def predict_label(self, df: pd.DataFrame, threshold: float = 0.5) -> pd.Series:\n",
    "        \"\"\"Label binaire selon un seuil.\"\"\"\n",
    "        print(\"[predict_label] → Début prédiction des labels...\")\n",
    "        p = self.predict_proba(df)\n",
    "        print(\"[predict_label] ✓ Fin prédiction des labels.\")\n",
    "        return pd.Series((p >= threshold).astype(int), index=df.index, name=f\"{self.target_col}_pred\")\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Charge un pipeline entraîné (préprocesseur inclus).\"\"\"\n",
    "        print(\"[load] → Chargement du modèle...\")\n",
    "        self.pipeline_ = joblib.load(path)\n",
    "        # on récupère le préprocesseur et la liste de features du pipeline sauvegardé\n",
    "        if hasattr(self.pipeline_, \"named_steps\") and \"preprocessor\" in self.pipeline_.named_steps:\n",
    "            self.preprocessor_ = self.pipeline_.named_steps[\"preprocessor\"]\n",
    "        # La feature_list_ est utile seulement pour _prepare_features (ordre des colonnes en entrée)\n",
    "        # On la reconstruit à partir des attributs de classe pour rester déterministe :\n",
    "        self.feature_list_ = (\n",
    "            [\"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "             \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"]\n",
    "            + [\"holiday_flag\", \"is_weekend\", \"operative\"]\n",
    "            + [\"station_id\"]\n",
    "        )\n",
    "        print(\"[load] ✓ Modèle chargé avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d96c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predictor.__init__] → Début initialisation du prédicteur\n",
      "[Predictor.__init__] ✓ Fin initialisation du prédicteur\n",
      "[load] → Chargement du modèle...\n",
      "[load] ✓ Modèle chargé avec succès.\n",
      "[predict_label] → Début prédiction des labels...\n",
      "[predict_proba] → Début prédiction des probabilités...\n",
      "[_prepare_features] → Préparation des features...\n",
      "[_add_time_features] → Début génération des features temporelles...\n",
      "[_add_time_features] ✓ Features temporelles ajoutées.\n",
      "[_build_preprocessor] → Construction du préprocesseur...\n",
      "[_build_preprocessor] ✓ Préprocesseur construit.\n",
      "[_ensure_columns] → Vérification des colonnes requises...\n",
      "[_ensure_columns] → Vérification des colonnes requises...\n",
      "[_ensure_columns] ✓ Toutes les colonnes sont présentes.\n",
      "[_prepare_features] ✓ Features préparées.\n",
      "[predict_proba] ✓ Fin prédiction des probabilités.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m pred = Predictor(timezone=\u001b[33m\"\u001b[39m\u001b[33mEurope/Paris\u001b[39m\u001b[33m\"\u001b[39m, target_col=\u001b[33m\"\u001b[39m\u001b[33mtarget_empty\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m pred.load(\u001b[33m\"\u001b[39m\u001b[33mvelib_model.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m df_out = \u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 316\u001b[39m, in \u001b[36mPredictor.predict_label\u001b[39m\u001b[34m(self, df, threshold)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Label binaire selon un seuil.\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[predict_label] → Début prédiction des labels...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m p = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[predict_label] ✓ Fin prédiction des labels.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series((p >= threshold).astype(\u001b[38;5;28mint\u001b[39m), index=df.index, name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.target_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mPredictor.predict_proba\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    309\u001b[39m X, _ = \u001b[38;5;28mself\u001b[39m._prepare_features(df, with_target=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    310\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[predict_proba] ✓ Fin prédiction des probabilités.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:904\u001b[39m, in \u001b[36mPipeline.predict_proba\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict_proba(Xt, **params)\n\u001b[32m    907\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1096\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1094\u001b[39m     routed_params = \u001b[38;5;28mself\u001b[39m._get_empty_routing()\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m Xs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_output(Xs)\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[32m   1106\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:897\u001b[39m, in \u001b[36mColumnTransformer._call_func_on_transformers\u001b[39m\u001b[34m(self, X, y, func, column_as_labels, routed_params)\u001b[39m\n\u001b[32m    885\u001b[39m             extra_args = {}\n\u001b[32m    886\u001b[39m         jobs.append(\n\u001b[32m    887\u001b[39m             delayed(func)(\n\u001b[32m    888\u001b[39m                 transformer=clone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[32m   (...)\u001b[39m\u001b[32m    894\u001b[39m             )\n\u001b[32m    895\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1520\u001b[39m, in \u001b[36m_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, params)\u001b[39m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_transform_one\u001b[39m(transformer, X, y, weight, params):\n\u001b[32m   1499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[32m   1500\u001b[39m \n\u001b[32m   1501\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1518\u001b[39m \u001b[33;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[32m   1519\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1521\u001b[39m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[32m   1522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1043\u001b[39m, in \u001b[36mOneHotEncoder.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1038\u001b[39m     warn_on_unknown = \u001b[38;5;28mself\u001b[39m.drop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m   1039\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1040\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minfrequent_if_exist\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1041\u001b[39m     }\n\u001b[32m   1042\u001b[39m     handle_unknown = \u001b[38;5;28mself\u001b[39m.handle_unknown\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m X_int, X_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m n_samples, n_features = X_int.shape\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:210\u001b[39m, in \u001b[36m_BaseEncoder._transform\u001b[39m\u001b[34m(self, X, handle_unknown, ensure_all_finite, warn_on_unknown, ignore_category_indices)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[32m    209\u001b[39m     Xi = X_list[i]\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     diff, valid_mask = \u001b[43m_check_unknown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategories_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(valid_mask):\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown == \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\_encode.py:313\u001b[39m, in \u001b[36m_check_unknown\u001b[39m\u001b[34m(values, known_values, return_mask)\u001b[39m\n\u001b[32m    310\u001b[39m         valid_mask = xp.ones(\u001b[38;5;28mlen\u001b[39m(values), dtype=xp.bool)\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xp.any(\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    314\u001b[39m     diff_is_nan = xp.isnan(diff)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xp.any(diff_is_nan):\n\u001b[32m    316\u001b[39m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# Entraîner et sauvegarder\n",
    "# trainer = ModelTrainer(model_type=\"gb\", target_col=\"target_empty\")\n",
    "# metrics = trainer.fit(df_train)\n",
    "# trainer.save(\"velib_model.joblib\")\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        # ==== Données départ (station de départ) ====\n",
    "        \"time\": \"2025-10-16T07:30:00Z\",   # Date/heure de départ (UTC)\n",
    "        \"station_id\": 75115034,           # ID station départ\n",
    "        \"capacity\": 40,                   # Capacité totale de la station\n",
    "        \"docks_available\": 24,           # Nombre de places libres\n",
    "        \"fill_rate\": 0.40,               # Ratio vélos présents / capacité\n",
    "        \"rolling_fill_rate\": 0.38,       # Moyenne mobile du fill_rate\n",
    "        \"holiday_flag\": 0,              # 1 si jour férié, sinon 0\n",
    "        \"is_weekend\": 0,               # 1 si week-end, sinon 0\n",
    "        \"operative\": 1,              # 1 si station opérationnelle, sinon 0\n",
    "\n",
    "        # # ==== Données arrivée (station d’arrivée) ====\n",
    "        # \"time\": \"2025-10-16T08:00:00Z\",     # Date/heure d’arrivée prévue\n",
    "        # \"station_id_arrival\": 75104021,            # ID station arrivée\n",
    "        # \"capacity_arrival\": 25,                    # Capacité totale station arrivée\n",
    "        # \"docks_available_arrival\": 10,            # Nombre de places libres à l’arrivée\n",
    "        # \"fill_rate_arrival\": 0.60,                # Ratio vélos présents / capacité\n",
    "        # \"rolling_fill_rate_arrival\": 0.57,       # Moyenne mobile du fill_rate\n",
    "        # \"holiday_flag_arrival\": 0,              # 1 si jour férié, sinon 0\n",
    "        # \"is_weekend_arrival\": 0,               # 1 si week-end, sinon 0\n",
    "        # \"operative_arrival\": 1,                # 1 si station opérationnelle, sinon 0\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Charger et prédire\n",
    "# pred = Predictor(timezone=\"Europe/Paris\", target_col=\"target_empty\")\n",
    "# pred.load(\"velib_model.joblib\")\n",
    "# df_out = pred.predict_label(pd.DataFrame(data), threshold=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
