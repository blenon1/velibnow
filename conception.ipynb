{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83458767",
   "metadata": {},
   "source": [
    "# Fonctions d'obtention des donn√©es\n",
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbcb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from config import URL_HOLIDAYS, URL_SCHOOL, URL_METEO_HOURLY\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66032f0",
   "metadata": {},
   "source": [
    "## Class de lecture des donn√©es velib historiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd96b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelibCsvReader:\n",
    "    def __init__(self):\n",
    "        self.file_path = \"./data/dataset/historique_stations.csv\"\n",
    "\n",
    "    def read_dataframe(self):\n",
    "        \"\"\"Combine status + info sur les stations\"\"\"\n",
    "        dataframe = pd.read_csv(\n",
    "            filepath_or_buffer=self.file_path,\n",
    "            sep=\",\"\n",
    "        )\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfbf7c",
   "metadata": {},
   "source": [
    "## Class API pour donn√©es sur les vacances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01808ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolidaysAPI:\n",
    "    \"\"\"\n",
    "    R√©cup√©ration des jours f√©ri√©s et des vacances scolaires fran√ßaises.\n",
    "    \"\"\"\n",
    "\n",
    "    def fetch_public_holidays(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = requests.get(URL_HOLIDAYS, timeout=10).json()\n",
    "            df = pd.DataFrame(list(data.items()), columns=[\"date\", \"holiday_name\"])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erreur API jours f√©ri√©s : {e}\")\n",
    "\n",
    "    def fetch_school_vacations(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = requests.get(URL_SCHOOL, timeout=10).json()\n",
    "            results = data.get(\"results\", [])\n",
    "            df = pd.DataFrame(results)\n",
    "            if not df.empty:\n",
    "                df = df[[\"start_date\", \"end_date\", \"zones\", \"description\"]]\n",
    "                df[\"start_date\"] = pd.to_datetime(df[\"start_date\"])\n",
    "                df[\"end_date\"] = pd.to_datetime(df[\"end_date\"])\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erreur API vacances scolaires : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee9993",
   "metadata": {},
   "source": [
    "## Class API pour les donn√©es m√©t√©o historiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7b4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "class WeatherCsvReader:\n",
    "    def __init__(self, file_path: str = \"./data/dataset/historical_meteo.csv\"):\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        # Mapping colonnes brutes -> colonnes standardis√©es\n",
    "        self.WEATHER_COL_MAP = {\n",
    "            \"AAAAMMJJHH\": \"timestamp\",\n",
    "            \"NUM_POSTE\":  \"station_id\",\n",
    "            \"NOM_USUEL\":  \"station_name\",\n",
    "            \"LAT\":        \"lat_deg\",\n",
    "            \"LON\":        \"lon_deg\",\n",
    "            \"ALTI\":       \"alt_m\",\n",
    "            \"RR1\":        \"precip_mm\",\n",
    "            \"DRR1\":       \"precip_dur_min\",\n",
    "            \"T\":          \"temp_c\",\n",
    "            \"TD\":         \"dewpoint_c\",\n",
    "            \"U\":          \"humidity_rel_pct\",\n",
    "            \"FF\":         \"wind_speed_10m_ms\",\n",
    "            \"DD\":         \"wind_dir_deg\",\n",
    "            \"FXI\":        \"wind_gust_10m_ms\",\n",
    "            \"PSTAT\":      \"pressure_hpa_station\",\n",
    "            \"PMER\":       \"pressure_hpa_sea\",\n",
    "\n",
    "            \"N\":          \"cloud_oktas\",\n",
    "            \"INS\":        \"insolation_min\",            # plan B si N absent\n",
    "            \"GLO\":        \"global_radiation_j_cm2\",    # plan B bis\n",
    "            \"WW\":         \"wmo_present_weather\",       # optionnel mais utile\n",
    "        }\n",
    "\n",
    "        # Sch√©ma final (cible) ‚Äî timestamp est pars√© √† part\n",
    "        self.WEATHER_DTYPES: Dict[str, str] = {\n",
    "            \"station_id\":          \"string\",\n",
    "            \"station_name\":        \"string\",\n",
    "            \"lat_deg\":             \"float64\",\n",
    "            \"lon_deg\":             \"float64\",\n",
    "            \"alt_m\":               \"Int64\",\n",
    "            \"timestamp\":           \"datetime64[ns]\",\n",
    "            \"precip_mm\":           \"float64\",\n",
    "            \"precip_dur_min\":      \"Int64\",\n",
    "            \"temp_c\":              \"float64\",\n",
    "            \"dewpoint_c\":          \"float64\",\n",
    "            \"humidity_rel_pct\":    \"float64\",\n",
    "            \"wind_speed_10m_ms\":   \"float64\",\n",
    "            \"wind_dir_deg\":        \"float64\",\n",
    "            \"wind_gust_10m_ms\":    \"float64\",\n",
    "            \"pressure_hpa\":        \"float64\",\n",
    "            \n",
    "            \"cloud_oktas\":   \"float64\",\n",
    "            \"insolation_min\":        \"float64\",\n",
    "            \"global_radiation_j_cm2\":    \"float64\",\n",
    "            \"wmo_present_weather\":        \"float64\",\n",
    "        }\n",
    "\n",
    "    def read_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Lecture brute du CSV (s√©parateur ';').\"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(self.file_path, sep=\";\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Erreur API m√©t√©o : {e}\")\n",
    "\n",
    "    def _select_rename(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Garde uniquement les colonnes connues puis les renomme (sans boucles).\"\"\"\n",
    "        cols_src = df.columns.intersection(self.WEATHER_COL_MAP.keys())\n",
    "        return df.loc[:, cols_src].rename(columns=self.WEATHER_COL_MAP)\n",
    "\n",
    "    def read_standardized(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Pipeline vectoris√© :\n",
    "        - select/rename\n",
    "        - parse timestamp (AAAAMMJJHH)\n",
    "        - coalesce pression (PSTAT prioritaire sur PMER)\n",
    "        - cast global selon sch√©ma\n",
    "        - ordre de colonnes propre\n",
    "        \"\"\"\n",
    "        df_raw = self.read_dataframe()\n",
    "        df = self._select_rename(df_raw)\n",
    "\n",
    "        # Parse timestamp (format AAAAMMJJHH)\n",
    "        df = df.assign(\n",
    "            timestamp=pd.to_datetime(\n",
    "                df[\"timestamp\"].astype(\"string\"),\n",
    "                format=\"%Y%m%d%H\",\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Coalesce pression (priorit√© station -> mer) si colonnes pr√©sentes\n",
    "        # (si absentes, bfill s'applique sur colonnes manquantes sans boucle)\n",
    "        pressure_sources = df.filter(items=[\"pressure_hpa_station\", \"pressure_hpa_sea\"])\n",
    "        if not pressure_sources.empty:\n",
    "            df[\"pressure_hpa\"] = pressure_sources.bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "        # Ordre + cast en une seule passe (les colonnes manquantes seront ajout√©es vides)\n",
    "        df = (\n",
    "            df\n",
    "            .reindex(columns=self.WEATHER_DTYPES.keys())  # ordre final\n",
    "            .astype(self.WEATHER_DTYPES, errors=\"ignore\")  # cast \"df.cast(schema)\" version pandas\n",
    "        )\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4119355",
   "metadata": {},
   "source": [
    "# Exploration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427cb21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es V√©los :\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "capacity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "available_mechanical",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "available_electrical",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "station_geo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "operative",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "5188a36f-f2d0-4251-9b44-6a0bee0d0c73",
       "rows": [
        [
         "0",
         "2020-11-26T12:59Z",
         "35",
         "4",
         "5",
         "Benjamin Godard - Victor Hugo",
         "48.86598,2.27572",
         "True"
        ],
        [
         "1",
         "2020-11-26T12:59Z",
         "55",
         "23",
         "4",
         "Andr√© Mazet - Saint-Andr√© des Arts",
         "48.85376,2.33910",
         "True"
        ],
        [
         "2",
         "2020-11-26T12:59Z",
         "20",
         "0",
         "0",
         "Charonne - Robert et Sonia Delauney",
         "48.85591,2.39257",
         "True"
        ],
        [
         "3",
         "2020-11-26T12:59Z",
         "21",
         "0",
         "1",
         "Toudouze - Clauzel",
         "48.87930,2.33736",
         "True"
        ],
        [
         "4",
         "2020-11-26T12:59Z",
         "30",
         "3",
         "1",
         "Mairie du 12√®me",
         "48.84086,2.38755",
         "True"
        ],
        [
         "5",
         "2020-11-26T12:59Z",
         "46",
         "18",
         "10",
         "Harpe - Saint-Germain",
         "48.85152,2.34367",
         "True"
        ],
        [
         "6",
         "2020-11-26T12:59Z",
         "60",
         "5",
         "2",
         "Jourdan - Stade Charl√©ty",
         "48.81943,2.34334",
         "True"
        ],
        [
         "7",
         "2020-11-26T12:59Z",
         "40",
         "15",
         "1",
         "Jouffroy d'Abbans - Wagram",
         "48.88197,2.30113",
         "True"
        ],
        [
         "8",
         "2020-11-26T12:59Z",
         "39",
         "12",
         "2",
         "Guersant - Gouvion-Saint-Cyr",
         "48.88288,2.28767",
         "True"
        ],
        [
         "9",
         "2020-11-26T12:59Z",
         "60",
         "2",
         "2",
         "Alibert - Jemmapes",
         "48.87104,2.36610",
         "True"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>capacity</th>\n",
       "      <th>available_mechanical</th>\n",
       "      <th>available_electrical</th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_geo</th>\n",
       "      <th>operative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Benjamin Godard - Victor Hugo</td>\n",
       "      <td>48.86598,2.27572</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>Andr√© Mazet - Saint-Andr√© des Arts</td>\n",
       "      <td>48.85376,2.33910</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Charonne - Robert et Sonia Delauney</td>\n",
       "      <td>48.85591,2.39257</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Toudouze - Clauzel</td>\n",
       "      <td>48.87930,2.33736</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mairie du 12√®me</td>\n",
       "      <td>48.84086,2.38755</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>Harpe - Saint-Germain</td>\n",
       "      <td>48.85152,2.34367</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Jourdan - Stade Charl√©ty</td>\n",
       "      <td>48.81943,2.34334</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Jouffroy d'Abbans - Wagram</td>\n",
       "      <td>48.88197,2.30113</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Guersant - Gouvion-Saint-Cyr</td>\n",
       "      <td>48.88288,2.28767</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-26T12:59Z</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Alibert - Jemmapes</td>\n",
       "      <td>48.87104,2.36610</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                time  capacity  available_mechanical  available_electrical  \\\n",
       "0  2020-11-26T12:59Z        35                     4                     5   \n",
       "1  2020-11-26T12:59Z        55                    23                     4   \n",
       "2  2020-11-26T12:59Z        20                     0                     0   \n",
       "3  2020-11-26T12:59Z        21                     0                     1   \n",
       "4  2020-11-26T12:59Z        30                     3                     1   \n",
       "5  2020-11-26T12:59Z        46                    18                    10   \n",
       "6  2020-11-26T12:59Z        60                     5                     2   \n",
       "7  2020-11-26T12:59Z        40                    15                     1   \n",
       "8  2020-11-26T12:59Z        39                    12                     2   \n",
       "9  2020-11-26T12:59Z        60                     2                     2   \n",
       "\n",
       "                          station_name       station_geo  operative  \n",
       "0        Benjamin Godard - Victor Hugo  48.86598,2.27572       True  \n",
       "1   Andr√© Mazet - Saint-Andr√© des Arts  48.85376,2.33910       True  \n",
       "2  Charonne - Robert et Sonia Delauney  48.85591,2.39257       True  \n",
       "3                   Toudouze - Clauzel  48.87930,2.33736       True  \n",
       "4                      Mairie du 12√®me  48.84086,2.38755       True  \n",
       "5                Harpe - Saint-Germain  48.85152,2.34367       True  \n",
       "6             Jourdan - Stade Charl√©ty  48.81943,2.34334       True  \n",
       "7           Jouffroy d'Abbans - Wagram  48.88197,2.30113       True  \n",
       "8         Guersant - Gouvion-Saint-Cyr  48.88288,2.28767       True  \n",
       "9                   Alibert - Jemmapes  48.87104,2.36610       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es vacances :\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "holiday_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_date",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "end_date",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "zones",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "a63e8930-f11e-4aa8-94a1-d2d3750e0ed2",
       "rows": [
        [
         "0",
         "2030-01-01 00:00:00",
         "1er janvier",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "2030-04-22 00:00:00",
         "Lundi de P√¢ques",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "2030-05-01 00:00:00",
         "1er mai",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "2030-05-08 00:00:00",
         "8 mai",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "2030-05-30 00:00:00",
         "Ascension",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "2030-06-10 00:00:00",
         "Lundi de Pentec√¥te",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "2030-07-14 00:00:00",
         "14 juillet",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "2030-08-15 00:00:00",
         "Assomption",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "2030-11-01 00:00:00",
         "Toussaint",
         "holiday",
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "2030-11-11 00:00:00",
         "11 novembre",
         "holiday",
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>zones</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030-01-01</td>\n",
       "      <td>1er janvier</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2030-04-22</td>\n",
       "      <td>Lundi de P√¢ques</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2030-05-01</td>\n",
       "      <td>1er mai</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2030-05-08</td>\n",
       "      <td>8 mai</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030-05-30</td>\n",
       "      <td>Ascension</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2030-06-10</td>\n",
       "      <td>Lundi de Pentec√¥te</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2030-07-14</td>\n",
       "      <td>14 juillet</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2030-08-15</td>\n",
       "      <td>Assomption</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2030-11-01</td>\n",
       "      <td>Toussaint</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2030-11-11</td>\n",
       "      <td>11 novembre</td>\n",
       "      <td>holiday</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date        holiday_name     type start_date end_date zones  \\\n",
       "0 2030-01-01         1er janvier  holiday        NaT      NaT   NaN   \n",
       "1 2030-04-22     Lundi de P√¢ques  holiday        NaT      NaT   NaN   \n",
       "2 2030-05-01             1er mai  holiday        NaT      NaT   NaN   \n",
       "3 2030-05-08               8 mai  holiday        NaT      NaT   NaN   \n",
       "4 2030-05-30           Ascension  holiday        NaT      NaT   NaN   \n",
       "5 2030-06-10  Lundi de Pentec√¥te  holiday        NaT      NaT   NaN   \n",
       "6 2030-07-14          14 juillet  holiday        NaT      NaT   NaN   \n",
       "7 2030-08-15          Assomption  holiday        NaT      NaT   NaN   \n",
       "8 2030-11-01           Toussaint  holiday        NaT      NaT   NaN   \n",
       "9 2030-11-11         11 novembre  holiday        NaT      NaT   NaN   \n",
       "\n",
       "  description  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "5         NaN  \n",
       "6         NaN  \n",
       "7         NaN  \n",
       "8         NaN  \n",
       "9         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es m√©t√©o :\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "station_id",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "station_name",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "lat_deg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon_deg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alt_m",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "precip_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_dur_min",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "temp_c",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dewpoint_c",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_rel_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_speed_10m_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_dir_deg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_gust_10m_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pressure_hpa",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloud_oktas",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "insolation_min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "global_radiation_j_cm2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wmo_present_weather",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "81ff0cfd-4903-435a-8415-183c314e74d7",
       "rows": [
        [
         "0",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 00:00:00",
         "0.0",
         "0",
         "0.7",
         "-3.3",
         "74.0",
         "8.9",
         "140.0",
         "14.0",
         "1013.8",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 01:00:00",
         "0.0",
         "0",
         "0.9",
         "-3.3",
         "74.0",
         "9.7",
         "150.0",
         "14.7",
         "1013.1",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "2",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 02:00:00",
         "0.0",
         "0",
         "0.7",
         "-2.2",
         "81.0",
         "10.2",
         "140.0",
         "14.3",
         "1012.6",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "3",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 03:00:00",
         "0.0",
         "20",
         "0.8",
         "-2.2",
         "81.0",
         "11.5",
         "130.0",
         "16.5",
         "1011.0",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "4",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 04:00:00",
         "0.0",
         "0",
         "0.9",
         "-2.0",
         "82.0",
         "12.9",
         "120.0",
         "18.0",
         "1008.7",
         "8.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "5",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 05:00:00",
         "0.0",
         "10",
         "0.5",
         "-1.2",
         "89.0",
         "13.7",
         "120.0",
         "20.2",
         "1006.4",
         "8.0",
         "0.0",
         "0.0",
         "70.0"
        ],
        [
         "6",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 06:00:00",
         "0.0",
         "60",
         "0.6",
         "-0.5",
         "92.0",
         "14.0",
         "120.0",
         "22.3",
         "1005.0",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ],
        [
         "7",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 07:00:00",
         "0.4",
         "60",
         "0.6",
         "0.0",
         "96.0",
         "14.6",
         "120.0",
         "22.1",
         "1003.6",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ],
        [
         "8",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 08:00:00",
         "1.2",
         "60",
         "0.8",
         "0.2",
         "96.0",
         "15.0",
         "120.0",
         "23.6",
         "1001.7",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ],
        [
         "9",
         "97502001",
         "ST-PIERRE",
         "46.766333",
         "-56.179167",
         "21",
         "2020-01-01 09:00:00",
         "5.1",
         "60",
         "0.9",
         "0.4",
         "97.0",
         "14.1",
         "110.0",
         "21.2",
         "1001.2",
         "8.0",
         "0.0",
         "0.0",
         "68.0"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>lat_deg</th>\n",
       "      <th>lon_deg</th>\n",
       "      <th>alt_m</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>precip_dur_min</th>\n",
       "      <th>temp_c</th>\n",
       "      <th>dewpoint_c</th>\n",
       "      <th>humidity_rel_pct</th>\n",
       "      <th>wind_speed_10m_ms</th>\n",
       "      <th>wind_dir_deg</th>\n",
       "      <th>wind_gust_10m_ms</th>\n",
       "      <th>pressure_hpa</th>\n",
       "      <th>cloud_oktas</th>\n",
       "      <th>insolation_min</th>\n",
       "      <th>global_radiation_j_cm2</th>\n",
       "      <th>wmo_present_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>140.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1013.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>150.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>1013.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>1012.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>120.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>89.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>120.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>1006.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>22.3</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 07:00:00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>60</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>120.0</td>\n",
       "      <td>22.1</td>\n",
       "      <td>1003.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 08:00:00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1001.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97502001</td>\n",
       "      <td>ST-PIERRE</td>\n",
       "      <td>46.766333</td>\n",
       "      <td>-56.179167</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-01-01 09:00:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>1001.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id station_name    lat_deg    lon_deg  alt_m           timestamp  \\\n",
       "0   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 00:00:00   \n",
       "1   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 01:00:00   \n",
       "2   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 02:00:00   \n",
       "3   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 03:00:00   \n",
       "4   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 04:00:00   \n",
       "5   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 05:00:00   \n",
       "6   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 06:00:00   \n",
       "7   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 07:00:00   \n",
       "8   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 08:00:00   \n",
       "9   97502001    ST-PIERRE  46.766333 -56.179167     21 2020-01-01 09:00:00   \n",
       "\n",
       "   precip_mm  precip_dur_min  temp_c  dewpoint_c  humidity_rel_pct  \\\n",
       "0        0.0               0     0.7        -3.3              74.0   \n",
       "1        0.0               0     0.9        -3.3              74.0   \n",
       "2        0.0               0     0.7        -2.2              81.0   \n",
       "3        0.0              20     0.8        -2.2              81.0   \n",
       "4        0.0               0     0.9        -2.0              82.0   \n",
       "5        0.0              10     0.5        -1.2              89.0   \n",
       "6        0.0              60     0.6        -0.5              92.0   \n",
       "7        0.4              60     0.6         0.0              96.0   \n",
       "8        1.2              60     0.8         0.2              96.0   \n",
       "9        5.1              60     0.9         0.4              97.0   \n",
       "\n",
       "   wind_speed_10m_ms  wind_dir_deg  wind_gust_10m_ms  pressure_hpa  \\\n",
       "0                8.9         140.0              14.0        1013.8   \n",
       "1                9.7         150.0              14.7        1013.1   \n",
       "2               10.2         140.0              14.3        1012.6   \n",
       "3               11.5         130.0              16.5        1011.0   \n",
       "4               12.9         120.0              18.0        1008.7   \n",
       "5               13.7         120.0              20.2        1006.4   \n",
       "6               14.0         120.0              22.3        1005.0   \n",
       "7               14.6         120.0              22.1        1003.6   \n",
       "8               15.0         120.0              23.6        1001.7   \n",
       "9               14.1         110.0              21.2        1001.2   \n",
       "\n",
       "   cloud_oktas  insolation_min  global_radiation_j_cm2  wmo_present_weather  \n",
       "0          8.0             0.0                     0.0                  0.0  \n",
       "1          8.0             0.0                     0.0                  0.0  \n",
       "2          8.0             0.0                     0.0                  0.0  \n",
       "3          8.0             0.0                     0.0                  0.0  \n",
       "4          8.0             0.0                     0.0                  0.0  \n",
       "5          8.0             0.0                     0.0                 70.0  \n",
       "6          8.0             0.0                     0.0                 68.0  \n",
       "7          8.0             0.0                     0.0                 68.0  \n",
       "8          8.0             0.0                     0.0                 68.0  \n",
       "9          8.0             0.0                     0.0                 68.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "velib_historical_dataframe = VelibCsvReader().read_dataframe()\n",
    "print(\"Donn√©es V√©los :\")\n",
    "display(velib_historical_dataframe.head(10))\n",
    "\n",
    "holidays_instance = HolidaysAPI()\n",
    "public_holidays_dataframe = holidays_instance.fetch_public_holidays()\n",
    "vacations_dataframe = holidays_instance.fetch_school_vacations()\n",
    "public_holidays_dataframe[\"type\"] = \"holiday\"\n",
    "vacations_dataframe[\"type\"] = \"vacation\"\n",
    "calendar_dataframe = pd.concat([public_holidays_dataframe, vacations_dataframe], ignore_index=True)\n",
    "print(\"Donn√©es vacances :\")\n",
    "display(calendar_dataframe.head(10))\n",
    "\n",
    "weather_dataframe = WeatherCsvReader().read_standardized()\n",
    "print(\"Donn√©es m√©t√©o :\")\n",
    "display(weather_dataframe.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f7988",
   "metadata": {},
   "source": [
    "# Mod√©lisation des donn√©es\n",
    "## Fonctions de mod√©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c1bf5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Pr√©paration des donn√©es...\n",
      "‚öôÔ∏è Construction des features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julie\\AppData\\Local\\Temp\\ipykernel_29840\\2493534093.py:70: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"fill_rate\"] = (df[\"available_total\"] / df[\"capacity\"].replace(0, pd.NA)).fillna(0).clip(0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Seuils appliqu√©s: target_empty >= 0.83 | target_full >= 0.17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureBuilder:\n",
    "    def __init__(self, velib, weather, calendar):\n",
    "        self.velib = velib\n",
    "        self.weather = weather\n",
    "        self.calendar = calendar\n",
    "\n",
    "    def _preprocess(self, velib, weather, calendar):\n",
    "        \"\"\"Pr√©pare et fusionne les datasets (fusion horaire simple)\"\"\"\n",
    "        print(\"üßπ Pr√©paration des donn√©es...\")\n",
    "\n",
    "        # Nettoyage des dates (UTC)\n",
    "        weather[\"timestamp\"] = pd.to_datetime(weather[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "        velib[\"time\"] = pd.to_datetime(velib[\"time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "        # Cl√© horaire\n",
    "        weather[\"ts_hour\"] = weather[\"timestamp\"].dt.floor(\"h\")\n",
    "        velib[\"ts_hour\"] = velib[\"time\"].dt.floor(\"h\")\n",
    "\n",
    "        # ===== Flags m√©t√©o tr√®s simples (0/1) =====\n",
    "        precip = pd.to_numeric(weather.get(\"precip_mm\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        precip_dur = pd.to_numeric(weather.get(\"precip_dur_min\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        ws = pd.to_numeric(weather.get(\"wind_speed_10m_ms\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        wg = pd.to_numeric(weather.get(\"wind_gust_10m_ms\", 0), errors=\"coerce\").fillna(0.0)\n",
    "        cloud_oktas = pd.to_numeric(weather.get(\"cloud_oktas\", np.nan), errors=\"coerce\")\n",
    "\n",
    "        # R√®gles binaires\n",
    "        weather[\"pluie\"]  = ((precip >= 0.1) | (precip_dur >= 5)).astype(int)\n",
    "        weather[\"vent\"]   = ((ws >= 8.0) | (wg >= 10.8)).astype(int)\n",
    "        weather[\"soleil\"] = cloud_oktas.le(2).fillna(False).astype(int)\n",
    "        weather[\"nuage\"]  = cloud_oktas.ge(6).fillna(False).astype(int)\n",
    "\n",
    "        # 1 ligne par heure\n",
    "        weather_flags = (\n",
    "            weather.sort_values(\"timestamp\")\n",
    "                   .drop_duplicates(subset=[\"ts_hour\"], keep=\"last\")\n",
    "                   [[\"ts_hour\", \"pluie\", \"vent\", \"soleil\", \"nuage\"]]\n",
    "                   .copy()\n",
    "        )\n",
    "        weather_flags[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]] = weather_flags[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]].fillna(0).astype(int)\n",
    "\n",
    "        # Fusion m√©t√©o ‚Üî V√©lib\n",
    "        df = velib.merge(weather_flags, on=\"ts_hour\", how=\"left\")\n",
    "        for c in [\"pluie\", \"vent\", \"soleil\", \"nuage\"]:\n",
    "            if c not in df.columns:\n",
    "                df[c] = 0\n",
    "        df[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]] = df[[\"pluie\", \"vent\", \"soleil\", \"nuage\"]].fillna(0).astype(int)\n",
    "\n",
    "        # Ajout calendrier\n",
    "        df[\"date\"] = df[\"time\"].dt.tz_convert(\"Europe/Paris\").dt.date\n",
    "        cal = calendar.copy()\n",
    "        cal[\"date\"] = pd.to_datetime(cal[\"start_date\"]).dt.date\n",
    "        cal[\"holiday_flag\"] = 1\n",
    "        df = df.merge(cal[[\"date\", \"holiday_flag\"]].drop_duplicates(), on=\"date\", how=\"left\")\n",
    "        df[\"holiday_flag\"] = df[\"holiday_flag\"].fillna(0).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _feature_engineering(self, df):\n",
    "        \"\"\"Cr√©e des variables d√©riv√©es utiles pour la pr√©diction\"\"\"\n",
    "        print(\"‚öôÔ∏è Construction des features...\")\n",
    "\n",
    "        # TOTAL v√©los dispo & bornes libres\n",
    "        df[\"available_total\"] = df[\"available_mechanical\"].fillna(0) + df[\"available_electrical\"].fillna(0)\n",
    "        df[\"docks_available\"] = df[\"capacity\"].fillna(0) - df[\"available_total\"]\n",
    "\n",
    "        # Taux d'occupation\n",
    "        df[\"fill_rate\"] = (df[\"available_total\"] / df[\"capacity\"].replace(0, pd.NA)).fillna(0).clip(0, 1)\n",
    "\n",
    "        # ‚úÖ Calcul des ratios s√ªrs\n",
    "        ratio_empty = (df[\"docks_available\"] / df[\"capacity\"].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0).clip(0, 1)\n",
    "        ratio_full = (df[\"available_total\"] / df[\"capacity\"].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0).clip(0, 1)\n",
    "\n",
    "        # ‚úÖ D√©finition automatique des seuils (adapt√©s √† ta distribution)\n",
    "        empty_threshold = ratio_empty.quantile(0.70)  # stations avec beaucoup de place\n",
    "        full_threshold  = ratio_full.quantile(0.30)   # stations avec assez de v√©los\n",
    "\n",
    "        print(f\"üìè Seuils appliqu√©s: target_empty >= {empty_threshold:.2f} | target_full >= {full_threshold:.2f}\")\n",
    "\n",
    "        # ‚úÖ Cibles binaires √©quilibr√©es\n",
    "        df[\"target_empty\"] = (ratio_empty >= empty_threshold).astype(int)\n",
    "        df[\"target_full\"]  = (ratio_full >= full_threshold).astype(int)\n",
    "\n",
    "        # Heures / jours\n",
    "        df = df.sort_values([\"station_name\", \"time\"])\n",
    "        df[\"hour\"] = df[\"time\"].dt.hour\n",
    "        df[\"day_of_week\"] = df[\"time\"].dt.day_name()\n",
    "\n",
    "        # Moyenne glissante (3h) par station\n",
    "        df[\"rolling_fill_rate\"] = (\n",
    "            df.groupby(\"station_name\")[\"fill_rate\"].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "        )\n",
    "\n",
    "        # Week-end\n",
    "        df[\"is_weekend\"] = df[\"day_of_week\"].isin([\"Saturday\", \"Sunday\"]).astype(int)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Ex√©cution compl√®te du pipeline de feature engineering\"\"\"\n",
    "        merged = self._preprocess(self.velib, self.weather, self.calendar)\n",
    "        if merged is None:\n",
    "            raise RuntimeError(\"[FeatureBuilder.run] _preprocess a renvoy√© None (attendu: DataFrame).\")\n",
    "        features = self._feature_engineering(merged)\n",
    "        return features\n",
    "\n",
    "\n",
    "# üß™ Exemple d'ex√©cution\n",
    "feature_dataframe = FeatureBuilder(\n",
    "    velib_historical_dataframe,\n",
    "    weather_dataframe,\n",
    "    calendar_dataframe\n",
    ").run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56712983",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d0d6a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Distribution finale de target_empty:\n",
      "                count  percent\n",
      "target_empty                  \n",
      "0             7672453    69.83\n",
      "1             3314277    30.17\n",
      "\n",
      "üìä Distribution finale de target_full:\n",
      "               count  percent\n",
      "target_full                  \n",
      "0            3252063     29.6\n",
      "1            7734667     70.4\n",
      "\n",
      "üìà Corr√©lation entre les deux cibles:\n",
      "              target_empty  target_full\n",
      "target_empty      1.000000    -0.958444\n",
      "target_full      -0.958444     1.000000\n"
     ]
    }
   ],
   "source": [
    "for col in [\"target_empty\", \"target_full\"]:\n",
    "    counts = feature_dataframe[col].value_counts().sort_index()\n",
    "    percents = feature_dataframe[col].value_counts(normalize=True).sort_index() * 100\n",
    "    print(f\"\\nüìä Distribution finale de {col}:\")\n",
    "    print(pd.DataFrame({\"count\": counts, \"percent\": percents.round(2)}))\n",
    "\n",
    "print(\"\\nüìà Corr√©lation entre les deux cibles:\")\n",
    "print(feature_dataframe[[\"target_empty\", \"target_full\"]].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72dbbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import joblib\n",
    "\n",
    "\n",
    "class VelibSimpleModel:\n",
    "    \"\"\"\n",
    "    Mod√®le simple et lisible pour pr√©dire si une station sera vide/pleine.\n",
    "\n",
    "    Principes :\n",
    "    - Utilise uniquement les colonnes d√©j√† pr√©sentes dans votre DataFrame d'exemple\n",
    "      (pas de m√©t√©o, pas de mapping).\n",
    "    - Encodage temporel cyclique (√† partir de 'time').\n",
    "    - Imputation l√©g√®re + standardisation pour les num√©riques, OHE pour station_id.\n",
    "    - API courte: fit / predict_proba / predict_label / save / load\n",
    "    - Param√®tres regroup√©s en dict (to_config / from_config).\n",
    "    \"\"\"\n",
    "\n",
    "    # Colonnes de base attendues (selon votre DataFrame d'entr√©e)\n",
    "    BASE_NUM_SCALED: List[str] = [\n",
    "        #\"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "        # features temporelles\n",
    "        \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\",\n",
    "    ]\n",
    "    BASE_BIN_PASSTHROUGH: List[str] = [\n",
    "        \"holiday_flag\", \"is_weekend\", \"operative\",\n",
    "        \"pluie\", \"vent\", \"soleil\", \"nuage\",\n",
    "    ]\n",
    "    BASE_CATEG: List[str] = [\"station_name\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str = \"target_empty\",        # ou \"target_full\"\n",
    "        model_type: str = \"gb\",                  # \"gb\" ou \"logit\"\n",
    "        timezone: str = \"Europe/Paris\",\n",
    "        random_state: int = 42,\n",
    "        test_size: float = 0.2,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        print(\"[__init__] ‚Üí D√©but initialisation du mod√®le\")\n",
    "        self.target_col = target_col\n",
    "        self.model_type = model_type\n",
    "        self.timezone = timezone\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Objets entra√Æn√©s\n",
    "        self.feature_list_: List[str] = []\n",
    "        self.preprocessor_: Optional[ColumnTransformer] = None\n",
    "        self.pipeline_: Optional[Pipeline] = None\n",
    "        print(\"[__init__] ‚úì Fin initialisation du mod√®le\")\n",
    "\n",
    "    # ------------- Helpers ---------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "        print(\"[_ensure_columns] ‚Üí V√©rification des colonnes requises...\")\n",
    "        missing = [c for c in cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "        print(\"[_ensure_columns] ‚úì Toutes les colonnes sont pr√©sentes.\")\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute hour/dow/month + encodage cyclique √† partir de la colonne 'time'.\n",
    "        On ne d√©pend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "        \"\"\"\n",
    "        print(\"[_add_time_features] ‚Üí D√©but g√©n√©ration des features temporelles...\")\n",
    "        if \"time\" not in df.columns:\n",
    "            raise ValueError(\"La colonne 'time' est requise.\")\n",
    "        ts = pd.to_datetime(df[\"time\"], utc=True).dt.tz_convert(self.timezone)\n",
    "\n",
    "        out = df.copy()\n",
    "        out[\"hour\"] = ts.dt.hour\n",
    "        out[\"dow\"] = ts.dt.weekday     # 0 = lundi\n",
    "        out[\"month\"] = ts.dt.month\n",
    "\n",
    "        out[\"hour_ssin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "        out[\"hour_ccos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "        out[\"dow_sin\"]   = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "        out[\"dow_cos\"]   = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "        print(\"[_add_time_features] ‚úì Features temporelles ajout√©es.\")\n",
    "        return out\n",
    "\n",
    "    def _build_preprocessor(self) -> None:\n",
    "        \"\"\"\n",
    "        Pr√©processeur:\n",
    "        - Num√©riques (imputation m√©diane + standardisation)\n",
    "        - Binaires (imputation la plus fr√©quente, pas de scaling)\n",
    "        - Cat√©gorielles (OHE handle_unknown='ignore')\n",
    "        \"\"\"\n",
    "        print(\"[_build_preprocessor] ‚Üí Construction du pr√©processeur...\")\n",
    "        # Gestion valeurs manquantes + standardisation\n",
    "        num_pipe = Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"sc\", StandardScaler()),\n",
    "        ])\n",
    "        # Gestion valeurs manquantes (pas scaler pour garder 0/1 lisible)\n",
    "        bin_pipe = Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        ])\n",
    "        # Transforme variables cat√©gorielles en binaire\n",
    "        cat_pipe = Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "            (\"ohe\", OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                dtype=np.float32,\n",
    "                sparse_output=True,\n",
    "            )),\n",
    "        ])\n",
    "\n",
    "        # Liste des features finales\n",
    "        self.feature_list_ = (\n",
    "            self.BASE_NUM_SCALED\n",
    "            + self.BASE_BIN_PASSTHROUGH\n",
    "            + self.BASE_CATEG\n",
    "        )\n",
    "\n",
    "        # Applique les 3 pipelines de transformation + drop autres colonnes\n",
    "        self.preprocessor_ = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_pipe, self.BASE_NUM_SCALED),\n",
    "                (\"bin\", bin_pipe, self.BASE_BIN_PASSTHROUGH),\n",
    "                (\"cat\", cat_pipe, self.BASE_CATEG),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            sparse_threshold=1.0,\n",
    "        )\n",
    "        print(\"[_build_preprocessor] ‚úì Pr√©processeur construit.\")\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, df: pd.DataFrame, with_target: bool\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        - Ajoute les features temporelles\n",
    "        - V√©rifie la pr√©sence des colonnes attendues\n",
    "        - Renvoie X (et y si with_target)\n",
    "        \"\"\"\n",
    "        print(\"[_prepare_features] ‚Üí Pr√©paration des features...\")\n",
    "        # Ajoute les features temporelles\n",
    "        df2 = self._add_time_features(df)\n",
    "\n",
    "        # Construire le pr√©processeur si pas encore fait\n",
    "        if self.preprocessor_ is None:\n",
    "            self._build_preprocessor()\n",
    "\n",
    "        # V√©rifier les colonnes d'entr√©e attendues\n",
    "        needed = (\n",
    "            set(self.BASE_NUM_SCALED + self.BASE_BIN_PASSTHROUGH + self.BASE_CATEG)\n",
    "            - {\"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"}  # cr√©√©es ici\n",
    "        )\n",
    "        self._ensure_columns(df2, sorted(needed))\n",
    "\n",
    "        X = df2[self.feature_list_] # Liste de colonne cr√©√©e dans _build_preprocessor()\n",
    "\n",
    "        y = None\n",
    "        if with_target: # V√©rification de la pr√©sence de la colonne cible (√† pr√©dire)\n",
    "            if self.target_col not in df2.columns:\n",
    "                raise ValueError(f\"Colonne cible manquante: '{self.target_col}'\")\n",
    "            y = df2[self.target_col].astype(int)\n",
    "\n",
    "        # Renvoi les colonnes de param√®tre ainsi que la colonne cible \n",
    "        print(\"[_prepare_features] ‚úì Features pr√©par√©es.\")\n",
    "        return X, y\n",
    "\n",
    "    # ------------- API publique ---------------\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Entra√Æne le mod√®le choisi et renvoie des m√©triques simples (AUC).\n",
    "        \"\"\"\n",
    "        print(\"[fit] ‚Üí D√©but entra√Ænement du mod√®le...\")\n",
    "        \n",
    "        self._build_preprocessor()# (r√©)initialise un pr√©processeur propre\n",
    "        X, y = self._prepare_features(df, with_target=True) # Pr√©paration des features\n",
    "\n",
    "        # Choix du classifieur\n",
    "        if self.model_type == \"logit\":\n",
    "            classifier = LogisticRegression(max_iter=300, solver=\"saga\", verbose=1, random_state=self.random_state)\n",
    "        else:\n",
    "            classifier = GradientBoostingClassifier(random_state=self.random_state, verbose=1)\n",
    "\n",
    "        # Assemblage pr√©processeur + classifieur\n",
    "        self.pipeline_ = Pipeline(steps=[(\"preprocessor\", self.preprocessor_), (\"classifier\", classifier)])\n",
    "\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.test_size,\n",
    "            stratify=y,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "        print(\"[fit] ‚Üí Entra√Ænement en cours...\")\n",
    "        self.pipeline_.fit(X_tr, y_tr)\n",
    "\n",
    "        print(\"[fit] ‚úì Entra√Ænement termin√©. √âvaluation en cours...\")\n",
    "\n",
    "        proba = self.pipeline_.predict_proba(X_va)[:, 1]\n",
    "        y_pred = (proba >= 0.5).astype(int)  # seuil simple\n",
    "\n",
    "        acc = accuracy_score(y_va, y_pred)\n",
    "        f1  = f1_score(y_va, y_pred)\n",
    "\n",
    "        metrics = {\n",
    "            \"val_accuracy\": float(acc),\n",
    "            \"val_f1\": float(f1),\n",
    "            \"n_samples_train\": int(len(X_tr)),\n",
    "            \"n_samples_val\": int(len(X_va)),\n",
    "        }\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[{self.target_col}] {self.model_type.upper()}  \"\n",
    "                f\"ACC={metrics['val_accuracy']:.3f}  F1={metrics['val_f1']:.3f}  \"\n",
    "                f\"(train={metrics['n_samples_train']}, val={metrics['n_samples_val']})\")\n",
    "\n",
    "\n",
    "        print(\"[fit] ‚úì Fin de l'entra√Ænement et des m√©triques.\")\n",
    "        return metrics\n",
    "\n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Probabilit√© d'√™tre positif (ex: vide si target_empty).\"\"\"\n",
    "        print(\"[predict_proba] ‚Üí D√©but pr√©diction des probabilit√©s...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        # NE PAS reconstruire le pr√©processeur ici\n",
    "        X, _ = self._prepare_features(df, with_target=False)\n",
    "        print(\"[predict_proba] ‚úì Fin pr√©diction des probabilit√©s.\")\n",
    "        return self.pipeline_.predict_proba(X)[:, 1]\n",
    "\n",
    "    def predict_label(self, df: pd.DataFrame, threshold: float = 0.5) -> pd.Series:\n",
    "        \"\"\"Label binaire selon un seuil.\"\"\"\n",
    "        print(\"[predict_label] ‚Üí D√©but pr√©diction des labels...\")\n",
    "        p = self.predict_proba(df)\n",
    "        print(\"[predict_label] ‚úì Fin pr√©diction des labels.\")\n",
    "        return pd.Series((p >= threshold).astype(int), index=df.index, name=f\"{self.target_col}_pred\")\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Sauvegarde le pipeline complet (pr√©traitement + mod√®le).\"\"\"\n",
    "        print(\"[save] ‚Üí Sauvegarde du mod√®le...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        joblib.dump(self.pipeline_, path)\n",
    "        print(\"[save] ‚úì Mod√®le sauvegard√©.\")\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Charge un pipeline entra√Æn√© (pr√©processeur inclus).\"\"\"\n",
    "        print(\"[load] ‚Üí Chargement du mod√®le...\")\n",
    "        self.pipeline_ = joblib.load(path)\n",
    "        # on r√©cup√®re le pr√©processeur et la liste de features du pipeline sauvegard√©\n",
    "        if hasattr(self.pipeline_, \"named_steps\") and \"preprocessor\" in self.pipeline_.named_steps:\n",
    "            self.preprocessor_ = self.pipeline_.named_steps[\"preprocessor\"]\n",
    "        # La feature_list_ est utile seulement pour _prepare_features (ordre des colonnes en entr√©e)\n",
    "        # On la reconstruit √† partir des attributs de classe pour rester d√©terministe :\n",
    "        self.feature_list_ = (\n",
    "            self.BASE_NUM_SCALED + self.BASE_BIN_PASSTHROUGH + self.BASE_CATEG\n",
    "        )\n",
    "        print(\"[load] ‚úì Mod√®le charg√© avec succ√®s.\")\n",
    "\n",
    "    # --------- Config dict ---------\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg: Dict) -> \"VelibSimpleModel\":\n",
    "        print(\"[from_config] ‚Üí Cr√©ation du mod√®le depuis un dictionnaire de config...\")\n",
    "        model = cls(**cfg)\n",
    "        print(\"[from_config] ‚úì Mod√®le cr√©√© depuis la config.\")\n",
    "        return model\n",
    "\n",
    "    def to_config(self) -> Dict:\n",
    "        print(\"[to_config] ‚Üí Export de la configuration du mod√®le...\")\n",
    "        cfg = {\n",
    "            \"target_col\": self.target_col,\n",
    "            \"model_type\": self.model_type,\n",
    "            \"timezone\": self.timezone,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"test_size\": self.test_size,\n",
    "            \"verbose\": self.verbose,\n",
    "        }\n",
    "        print(\"[to_config] ‚úì Configuration export√©e.\")\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cb6b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[from_config] ‚Üí Cr√©ation du mod√®le depuis un dictionnaire de config...\n",
      "[__init__] ‚Üí D√©but initialisation du mod√®le\n",
      "[__init__] ‚úì Fin initialisation du mod√®le\n",
      "[from_config] ‚úì Mod√®le cr√©√© depuis la config.\n",
      "[fit] ‚Üí D√©but entra√Ænement du mod√®le...\n",
      "[_build_preprocessor] ‚Üí Construction du pr√©processeur...\n",
      "[_build_preprocessor] ‚úì Pr√©processeur construit.\n",
      "[_prepare_features] ‚Üí Pr√©paration des features...\n",
      "[_add_time_features] ‚Üí D√©but g√©n√©ration des features temporelles...\n",
      "[_add_time_features] ‚úì Features temporelles ajout√©es.\n",
      "[_ensure_columns] ‚Üí V√©rification des colonnes requises...\n",
      "[_ensure_columns] ‚úì Toutes les colonnes sont pr√©sentes.\n",
      "[_prepare_features] ‚úì Features pr√©par√©es.\n",
      "[fit] ‚Üí Entra√Ænement en cours...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2182           31.98m\n",
      "         2           1.2131           31.16m\n",
      "         3           1.2089           31.14m\n",
      "         4           1.2055           30.70m\n",
      "         5           1.2026           30.21m\n",
      "         6           1.2002           29.90m\n",
      "         7           1.1980           29.66m\n",
      "         8           1.1963           29.94m\n",
      "         9           1.1948           29.41m\n",
      "        10           1.1933           28.88m\n",
      "        20           1.1860           25.36m\n",
      "        30           1.1811           21.99m\n",
      "        40           1.1770           18.77m\n",
      "        50           1.1730           15.24m\n",
      "        60           1.1697           11.99m\n",
      "        70           1.1665            8.87m\n",
      "        80           1.1635            5.86m\n",
      "        90           1.1607            2.91m\n",
      "       100           1.1579            0.00s\n",
      "[fit] ‚úì Entra√Ænement termin√©. √âvaluation en cours...\n",
      "[target_empty] GB  ACC=0.718  F1=0.130  (train=8789384, val=2197346)\n",
      "[fit] ‚úì Fin de l'entra√Ænement et des m√©triques.\n",
      "üìä R√©sultats de l'entra√Ænement :\n",
      "  - val_accuracy: 0.7178127613948827\n",
      "  - val_f1: 0.12989123342225314\n",
      "  - n_samples_train: 8789384\n",
      "  - n_samples_val: 2197346\n",
      "[save] ‚Üí Sauvegarde du mod√®le...\n",
      "[save] ‚úì Mod√®le sauvegard√©.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Charger vos donn√©es\n",
    "df = feature_dataframe # pd.read_csv(\"velib_data.csv\")  # ou df = votre_dataframe d√©j√† charg√©\n",
    "\n",
    "# 2. Cr√©er le mod√®le avec vos param√®tres\n",
    "config = {\n",
    "    \"target_col\": \"target_empty\",     # ou \"target_full\"\n",
    "    \"model_type\": \"gb\",               # \"gb\" ou \"logit\"\n",
    "    \"timezone\": \"Europe/Paris\",\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.2,\n",
    "    \"verbose\": True\n",
    "}\n",
    "model = VelibSimpleModel.from_config(config)\n",
    "\n",
    "# 3. Lancer l'entra√Ænement\n",
    "metrics = model.fit(feature_dataframe)\n",
    "\n",
    "# 4. Afficher les r√©sultats\n",
    "print(\"üìä R√©sultats de l'entra√Ænement :\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "# 5. (Optionnel) Sauvegarder le mod√®le\n",
    "model.save(\"velib_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6902c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_velib.py\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# --- Helpers communs ---------------------------------------------------------\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "    print(\"[_ensure_columns] ‚Üí V√©rification des colonnes requises...\")\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "    print(\"[_ensure_columns] ‚úì Toutes les colonnes sont pr√©sentes.\")\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, timezone: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute hour/dow/month + encodage cyclique √† partir de la colonne 'time'.\n",
    "    On ne d√©pend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "    \"\"\"\n",
    "    print(\"[_add_time_features] ‚Üí D√©but g√©n√©ration des features temporelles...\")\n",
    "    if \"time\" not in df.columns:\n",
    "        raise ValueError(\"La colonne 'time' est requise.\")\n",
    "    ts = pd.to_datetime(df[\"time\"], utc=True).dt.tz_convert(timezone)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"hour\"] = ts.dt.hour\n",
    "    out[\"dow\"] = ts.dt.weekday     # 0 = lundi\n",
    "    out[\"month\"] = ts.dt.month\n",
    "\n",
    "    out[\"hour_ssin\"] = np.sin(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"hour_ccos\"] = np.cos(2 * np.pi * out[\"hour\"] / 24)\n",
    "    out[\"dow_sin\"]   = np.sin(2 * np.pi * out[\"dow\"] / 7)\n",
    "    out[\"dow_cos\"]   = np.cos(2 * np.pi * out[\"dow\"] / 7)\n",
    "    print(\"[_add_time_features] ‚úì Features temporelles ajout√©es.\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_preprocessor(\n",
    "    BASE_NUM_SCALED: List[str],\n",
    "    BASE_BIN_PASSTHROUGH: List[str],\n",
    "    BASE_CATEG: List[str],\n",
    ") -> Tuple[ColumnTransformer, List[str]]:\n",
    "    \"\"\"\n",
    "    Pr√©processeur:\n",
    "    - Num√©riques (imputation m√©diane + standardisation)\n",
    "    - Binaires (imputation la plus fr√©quente, pas de scaling)\n",
    "    - Cat√©gorielles (OHE handle_unknown='ignore')\n",
    "    \"\"\"\n",
    "    print(\"[_build_preprocessor] ‚Üí Construction du pr√©processeur...\")\n",
    "    # Gestion valeurs manquantes + standardisation\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\", StandardScaler()),\n",
    "    ])\n",
    "    # Gestion valeurs manquantes (pas scaler pour garder 0/1 lisible)\n",
    "    bin_pipe = Pipeline(steps=[\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ])\n",
    "    # Transforme variables cat√©gorielles en binaire\n",
    "    cat_pipe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "    # Liste des features finales\n",
    "    feature_list = BASE_NUM_SCALED + BASE_BIN_PASSTHROUGH + BASE_CATEG\n",
    "\n",
    "    # Applique les 3 pipelines de transformation + drop autres colonnes\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, BASE_NUM_SCALED),\n",
    "            (\"bin\", bin_pipe, BASE_BIN_PASSTHROUGH),\n",
    "            (\"cat\", cat_pipe, BASE_CATEG),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "    print(\"[_build_preprocessor] ‚úì Pr√©processeur construit.\")\n",
    "    return preprocessor, feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94856e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_trainer_and_predictor.py\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import joblib\n",
    "\n",
    "# <<< NEW: utils communs >>>\n",
    "# from utils_velib import ensure_columns as _utils_ensure_columns\n",
    "# from utils_velib import add_time_features as _utils_add_time_features\n",
    "# from utils_velib import build_preprocessor as _utils_build_preprocessor\n",
    "\n",
    "\n",
    "# ============================\n",
    "#   ENTRA√éNEMENT : ModelTrainer\n",
    "# ============================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Mod√®le simple et lisible pour pr√©dire si une station sera vide/pleine.\n",
    "\n",
    "    Principes :\n",
    "    - Utilise uniquement les colonnes d√©j√† pr√©sentes dans votre DataFrame d'exemple\n",
    "      (pas de m√©t√©o, pas de mapping).\n",
    "    - Encodage temporel cyclique (√† partir de 'time').\n",
    "    - Imputation l√©g√®re + standardisation pour les num√©riques, OHE pour station_id.\n",
    "    - API courte: fit / predict_proba / predict_label / save / load\n",
    "    - Param√®tres regroup√©s en dict (to_config / from_config).\n",
    "    \"\"\"\n",
    "\n",
    "    # Colonnes de base attendues (selon votre DataFrame d'entr√©e)\n",
    "    BASE_NUM_SCALED: List[str] = [\n",
    "        \"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "        # features temporelles\n",
    "        \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\",\n",
    "    ]\n",
    "    BASE_BIN_PASSTHROUGH: List[str] = [\"holiday_flag\", \"is_weekend\", \"operative\"]\n",
    "    BASE_CATEG: List[str] = [\"station_id\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_col: str = \"target_empty\",        # ou \"target_full\"\n",
    "        model_type: str = \"gb\",                  # \"gb\" ou \"logit\"\n",
    "        timezone: str = \"Europe/Paris\",\n",
    "        random_state: int = 42,\n",
    "        test_size: float = 0.2,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        print(\"[__init__] ‚Üí D√©but initialisation du mod√®le\")\n",
    "        self.target_col = target_col\n",
    "        self.model_type = model_type\n",
    "        self.timezone = timezone\n",
    "        self.random_state = random_state\n",
    "        self.test_size = test_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Objets entra√Æn√©s\n",
    "        self.feature_list_: List[str] = []\n",
    "        self.preprocessor_: Optional[ColumnTransformer] = None\n",
    "        self.pipeline_: Optional[Pipeline] = None\n",
    "        print(\"[__init__] ‚úì Fin initialisation du mod√®le\")\n",
    "\n",
    "    # ------------- Helpers ---------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "        print(\"[_ensure_columns] ‚Üí V√©rification des colonnes requises...\")\n",
    "        ensure_columns(df, cols)\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute hour/dow/month + encodage cyclique √† partir de la colonne 'time'.\n",
    "        On ne d√©pend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "        \"\"\"\n",
    "        return add_time_features(df, self.timezone)\n",
    "\n",
    "    def _build_preprocessor(self) -> None:\n",
    "        \"\"\"\n",
    "        Pr√©processeur:\n",
    "        - Num√©riques (imputation m√©diane + standardisation)\n",
    "        - Binaires (imputation la plus fr√©quente, pas de scaling)\n",
    "        - Cat√©gorielles (OHE handle_unknown='ignore')\n",
    "        \"\"\"\n",
    "        self.preprocessor_, self.feature_list_ = build_preprocessor(\n",
    "            self.BASE_NUM_SCALED, self.BASE_BIN_PASSTHROUGH, self.BASE_CATEG\n",
    "        )\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, df: pd.DataFrame, with_target: bool\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        - Ajoute les features temporelles\n",
    "        - V√©rifie la pr√©sence des colonnes attendues\n",
    "        - Renvoie X (et y si with_target)\n",
    "        \"\"\"\n",
    "        print(\"[_prepare_features] ‚Üí Pr√©paration des features...\")\n",
    "        # Ajoute les features temporelles\n",
    "        df2 = self._add_time_features(df)\n",
    "\n",
    "        # Construire le pr√©processeur si pas encore fait\n",
    "        if self.preprocessor_ is None:\n",
    "            self._build_preprocessor()\n",
    "\n",
    "        # V√©rifier les colonnes d'entr√©e attendues\n",
    "        needed = (\n",
    "            set(self.BASE_NUM_SCALED + self.BASE_BIN_PASSTHROUGH + self.BASE_CATEG)\n",
    "            - {\"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"}  # cr√©√©es ici\n",
    "        )\n",
    "        self._ensure_columns(df2, sorted(needed))\n",
    "\n",
    "        X = df2[self.feature_list_] # Liste de colonne cr√©√©e dans _build_preprocessor()\n",
    "\n",
    "        y = None\n",
    "        if with_target: # V√©rification de la pr√©sence de la colonne cible (√† pr√©dire)\n",
    "            if self.target_col not in df2.columns:\n",
    "                raise ValueError(f\"Colonne cible manquante: '{self.target_col}'\")\n",
    "            y = df2[self.target_col].astype(int)\n",
    "\n",
    "        # Renvoi les colonnes de param√®tre ainsi que la colonne cible \n",
    "        print(\"[_prepare_features] ‚úì Features pr√©par√©es.\")\n",
    "        return X, y\n",
    "\n",
    "    # ------------- API publique ---------------\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Entra√Æne le mod√®le choisi et renvoie des m√©triques simples (AUC).\n",
    "        \"\"\"\n",
    "        print(\"[fit] ‚Üí D√©but entra√Ænement du mod√®le...\")\n",
    "        \n",
    "        self._build_preprocessor()# (r√©)initialise un pr√©processeur propre\n",
    "        X, y = self._prepare_features(df, with_target=True) # Pr√©paration des features\n",
    "\n",
    "        # Choix du classifieur\n",
    "        if self.model_type == \"logit\":\n",
    "            classifier = LogisticRegression(max_iter=300, solver=\"saga\", verbose=1, random_state=self.random_state)\n",
    "        else:\n",
    "            classifier = GradientBoostingClassifier(random_state=self.random_state, verbose=1)\n",
    "\n",
    "        # Assemblage pr√©processeur + classifieur\n",
    "        self.pipeline_ = Pipeline(steps=[(\"preprocessor\", self.preprocessor_), (\"classifier\", classifier)])\n",
    "\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.test_size,\n",
    "            stratify=y,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "        print(\"[fit] ‚Üí Entra√Ænement en cours...\")\n",
    "        self.pipeline_.fit(X_tr, y_tr)\n",
    "        print(\"[fit] ‚úì Entra√Ænement termin√©. √âvaluation en cours...\")\n",
    "\n",
    "        proba = self.pipeline_.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_va, proba)\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "\n",
    "        metrics = {\n",
    "            \"val_auc\": float(auc),\n",
    "            \"n_samples_train\": int(len(X_tr)),\n",
    "            \"n_samples_val\": int(len(X_va)),\n",
    "        }\n",
    "        if self.verbose:\n",
    "            print(f\"[{self.target_col}] {self.model_type.upper()}  \"\n",
    "                  f\"AUC={metrics['val_auc']:.3f}  \"\n",
    "                  f\"(train={metrics['n_samples_train']}, val={metrics['n_samples_val']})\")\n",
    "\n",
    "        print(\"[fit] ‚úì Fin de l'entra√Ænement et des m√©triques.\")\n",
    "        return metrics\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Sauvegarde le pipeline complet (pr√©traitement + mod√®le).\"\"\"\n",
    "        print(\"[save] ‚Üí Sauvegarde du mod√®le...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        joblib.dump(self.pipeline_, path)\n",
    "        print(\"[save] ‚úì Mod√®le sauvegard√©.\")\n",
    "\n",
    "    # --------- Config dict ---------\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg: Dict) -> \"ModelTrainer\":\n",
    "        print(\"[from_config] ‚Üí Cr√©ation du mod√®le depuis un dictionnaire de config...\")\n",
    "        model = cls(**cfg)\n",
    "        print(\"[from_config] ‚úì Mod√®le cr√©√© depuis la config.\")\n",
    "        return model\n",
    "\n",
    "    def to_config(self) -> Dict:\n",
    "        print(\"[to_config] ‚Üí Export de la configuration du mod√®le...\")\n",
    "        cfg = {\n",
    "            \"target_col\": self.target_col,\n",
    "            \"model_type\": self.model_type,\n",
    "            \"timezone\": self.timezone,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"test_size\": self.test_size,\n",
    "            \"verbose\": self.verbose,\n",
    "        }\n",
    "        print(\"[to_config] ‚úì Configuration export√©e.\")\n",
    "        return cfg\n",
    "\n",
    "\n",
    "# ============================\n",
    "#   PR√âDICTION : Predictor\n",
    "# ============================\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Pr√©dicteur bas√© sur un pipeline entra√Æn√©.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timezone: str = \"Europe/Paris\", target_col: str = \"target_empty\", verbose: bool = True):\n",
    "        print(\"[Predictor.__init__] ‚Üí D√©but initialisation du pr√©dicteur\")\n",
    "        self.timezone = timezone\n",
    "        self.target_col = target_col\n",
    "        self.verbose = verbose\n",
    "        self.feature_list_: List[str] = []\n",
    "        self.preprocessor_: Optional[ColumnTransformer] = None\n",
    "        self.pipeline_: Optional[Pipeline] = None\n",
    "        print(\"[Predictor.__init__] ‚úì Fin initialisation du pr√©dicteur\")\n",
    "\n",
    "    # ------------- Helpers ---------------\n",
    "    # (fonctions conserv√©es telles quelles pour pr√©parer X comme √† l'entra√Ænement)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_columns(df: pd.DataFrame, cols: List[str]) -> None:\n",
    "        print(\"[_ensure_columns] ‚Üí V√©rification des colonnes requises...\")\n",
    "        ensure_columns(df, cols)\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute hour/dow/month + encodage cyclique √† partir de la colonne 'time'.\n",
    "        On ne d√©pend pas des colonnes 'hour'/'day_of_week' existantes pour garder la logique simple et robuste.\n",
    "        \"\"\"\n",
    "        return add_time_features(df, self.timezone)\n",
    "\n",
    "    def _build_preprocessor(self) -> None:\n",
    "        \"\"\"\n",
    "        Pr√©processeur:\n",
    "        - Num√©riques (imputation m√©diane + standardisation)\n",
    "        - Binaires (imputation la plus fr√©quente, pas de scaling)\n",
    "        - Cat√©gorielles (OHE handle_unknown='ignore')\n",
    "        \"\"\"\n",
    "        # Remarque : on reprend les m√™mes noms que c√¥t√© entra√Ænement\n",
    "        BASE_NUM_SCALED = [\n",
    "            \"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "            \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\",\n",
    "        ]\n",
    "        BASE_BIN_PASSTHROUGH = [\"holiday_flag\", \"is_weekend\", \"operative\"]\n",
    "        BASE_CATEG = [\"station_id\"]\n",
    "\n",
    "        self.preprocessor_, self.feature_list_ = build_preprocessor(\n",
    "            BASE_NUM_SCALED, BASE_BIN_PASSTHROUGH, BASE_CATEG\n",
    "        )\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, df: pd.DataFrame, with_target: bool\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        - Ajoute les features temporelles\n",
    "        - V√©rifie la pr√©sence des colonnes attendues\n",
    "        - Renvoie X (et y si with_target)\n",
    "        \"\"\"\n",
    "        print(\"[_prepare_features] ‚Üí Pr√©paration des features...\")\n",
    "        # Ajoute les features temporelles\n",
    "        df2 = self._add_time_features(df)\n",
    "\n",
    "        # Construire le pr√©processeur si pas encore fait\n",
    "        if self.preprocessor_ is None:\n",
    "            self._build_preprocessor()\n",
    "\n",
    "        # V√©rifier les colonnes d'entr√©e attendues\n",
    "        needed = (\n",
    "            set(self.feature_list_)  # m√™mes colonnes qu'√† l'entra√Ænement\n",
    "            - {\"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"}  # cr√©√©es ici\n",
    "        )\n",
    "        self._ensure_columns(df2, sorted(needed))\n",
    "\n",
    "        X = df2[self.feature_list_] # Liste de colonne cr√©√©e dans _build_preprocessor()\n",
    "\n",
    "        y = None\n",
    "        if with_target: # V√©rification de la pr√©sence de la colonne cible (√† pr√©dire)\n",
    "            if self.target_col not in df2.columns:\n",
    "                raise ValueError(f\"Colonne cible manquante: '{self.target_col}'\")\n",
    "            y = df2[self.target_col].astype(int)\n",
    "\n",
    "        # Renvoi les colonnes de param√®tre ainsi que la colonne cible \n",
    "        print(\"[_prepare_features] ‚úì Features pr√©par√©es.\")\n",
    "        return X, y\n",
    "\n",
    "    # ------------- API publique ---------------\n",
    "\n",
    "    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Probabilit√© d'√™tre positif (ex: vide si target_empty).\"\"\"\n",
    "        print(\"[predict_proba] ‚Üí D√©but pr√©diction des probabilit√©s...\")\n",
    "        check_is_fitted(self.pipeline_, \"named_steps\")\n",
    "        # NE PAS reconstruire le pr√©processeur ici\n",
    "        X, _ = self._prepare_features(df, with_target=False)\n",
    "        print(\"[predict_proba] ‚úì Fin pr√©diction des probabilit√©s.\")\n",
    "        return self.pipeline_.predict_proba(X)[:, 1]\n",
    "\n",
    "    def predict_label(self, df: pd.DataFrame, threshold: float = 0.5) -> pd.Series:\n",
    "        \"\"\"Label binaire selon un seuil.\"\"\"\n",
    "        print(\"[predict_label] ‚Üí D√©but pr√©diction des labels...\")\n",
    "        p = self.predict_proba(df)\n",
    "        print(\"[predict_label] ‚úì Fin pr√©diction des labels.\")\n",
    "        return pd.Series((p >= threshold).astype(int), index=df.index, name=f\"{self.target_col}_pred\")\n",
    "\n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Charge un pipeline entra√Æn√© (pr√©processeur inclus).\"\"\"\n",
    "        print(\"[load] ‚Üí Chargement du mod√®le...\")\n",
    "        self.pipeline_ = joblib.load(path)\n",
    "        # on r√©cup√®re le pr√©processeur et la liste de features du pipeline sauvegard√©\n",
    "        if hasattr(self.pipeline_, \"named_steps\") and \"preprocessor\" in self.pipeline_.named_steps:\n",
    "            self.preprocessor_ = self.pipeline_.named_steps[\"preprocessor\"]\n",
    "        # La feature_list_ est utile seulement pour _prepare_features (ordre des colonnes en entr√©e)\n",
    "        # On la reconstruit √† partir des attributs de classe pour rester d√©terministe :\n",
    "        self.feature_list_ = (\n",
    "            [\"capacity\", \"docks_available\", \"fill_rate\", \"rolling_fill_rate\",\n",
    "             \"hour_ssin\", \"hour_ccos\", \"dow_sin\", \"dow_cos\", \"month\"]\n",
    "            + [\"holiday_flag\", \"is_weekend\", \"operative\"]\n",
    "            + [\"station_id\"]\n",
    "        )\n",
    "        print(\"[load] ‚úì Mod√®le charg√© avec succ√®s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d96c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Predictor.__init__] ‚Üí D√©but initialisation du pr√©dicteur\n",
      "[Predictor.__init__] ‚úì Fin initialisation du pr√©dicteur\n",
      "[load] ‚Üí Chargement du mod√®le...\n",
      "[load] ‚úì Mod√®le charg√© avec succ√®s.\n",
      "[predict_label] ‚Üí D√©but pr√©diction des labels...\n",
      "[predict_proba] ‚Üí D√©but pr√©diction des probabilit√©s...\n",
      "[_prepare_features] ‚Üí Pr√©paration des features...\n",
      "[_add_time_features] ‚Üí D√©but g√©n√©ration des features temporelles...\n",
      "[_add_time_features] ‚úì Features temporelles ajout√©es.\n",
      "[_build_preprocessor] ‚Üí Construction du pr√©processeur...\n",
      "[_build_preprocessor] ‚úì Pr√©processeur construit.\n",
      "[_ensure_columns] ‚Üí V√©rification des colonnes requises...\n",
      "[_ensure_columns] ‚Üí V√©rification des colonnes requises...\n",
      "[_ensure_columns] ‚úì Toutes les colonnes sont pr√©sentes.\n",
      "[_prepare_features] ‚úì Features pr√©par√©es.\n",
      "[predict_proba] ‚úì Fin pr√©diction des probabilit√©s.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m pred = Predictor(timezone=\u001b[33m\"\u001b[39m\u001b[33mEurope/Paris\u001b[39m\u001b[33m\"\u001b[39m, target_col=\u001b[33m\"\u001b[39m\u001b[33mtarget_empty\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m pred.load(\u001b[33m\"\u001b[39m\u001b[33mvelib_model.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m df_out = \u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 316\u001b[39m, in \u001b[36mPredictor.predict_label\u001b[39m\u001b[34m(self, df, threshold)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Label binaire selon un seuil.\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[predict_label] ‚Üí D√©but pr√©diction des labels...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m p = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[predict_label] ‚úì Fin pr√©diction des labels.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series((p >= threshold).astype(\u001b[38;5;28mint\u001b[39m), index=df.index, name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.target_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mPredictor.predict_proba\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    309\u001b[39m X, _ = \u001b[38;5;28mself\u001b[39m._prepare_features(df, with_target=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    310\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[predict_proba] ‚úì Fin pr√©diction des probabilit√©s.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:904\u001b[39m, in \u001b[36mPipeline.predict_proba\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict_proba(Xt, **params)\n\u001b[32m    907\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1096\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1094\u001b[39m     routed_params = \u001b[38;5;28mself\u001b[39m._get_empty_routing()\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m Xs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_output(Xs)\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[32m   1106\u001b[39m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:897\u001b[39m, in \u001b[36mColumnTransformer._call_func_on_transformers\u001b[39m\u001b[34m(self, X, y, func, column_as_labels, routed_params)\u001b[39m\n\u001b[32m    885\u001b[39m             extra_args = {}\n\u001b[32m    886\u001b[39m         jobs.append(\n\u001b[32m    887\u001b[39m             delayed(func)(\n\u001b[32m    888\u001b[39m                 transformer=clone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[32m   (...)\u001b[39m\u001b[32m    894\u001b[39m             )\n\u001b[32m    895\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1520\u001b[39m, in \u001b[36m_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, params)\u001b[39m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_transform_one\u001b[39m(transformer, X, y, weight, params):\n\u001b[32m   1499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[32m   1500\u001b[39m \n\u001b[32m   1501\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1518\u001b[39m \u001b[33;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[32m   1519\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1521\u001b[39m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[32m   1522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1043\u001b[39m, in \u001b[36mOneHotEncoder.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1038\u001b[39m     warn_on_unknown = \u001b[38;5;28mself\u001b[39m.drop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m   1039\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1040\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minfrequent_if_exist\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1041\u001b[39m     }\n\u001b[32m   1042\u001b[39m     handle_unknown = \u001b[38;5;28mself\u001b[39m.handle_unknown\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m X_int, X_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m n_samples, n_features = X_int.shape\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:210\u001b[39m, in \u001b[36m_BaseEncoder._transform\u001b[39m\u001b[34m(self, X, handle_unknown, ensure_all_finite, warn_on_unknown, ignore_category_indices)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[32m    209\u001b[39m     Xi = X_list[i]\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     diff, valid_mask = \u001b[43m_check_unknown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategories_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(valid_mask):\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown == \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\julie\\Documents\\M2\\Hackaton\\velibnow\\.venv\\Lib\\site-packages\\sklearn\\utils\\_encode.py:313\u001b[39m, in \u001b[36m_check_unknown\u001b[39m\u001b[34m(values, known_values, return_mask)\u001b[39m\n\u001b[32m    310\u001b[39m         valid_mask = xp.ones(\u001b[38;5;28mlen\u001b[39m(values), dtype=xp.bool)\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xp.any(\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    314\u001b[39m     diff_is_nan = xp.isnan(diff)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xp.any(diff_is_nan):\n\u001b[32m    316\u001b[39m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# Entra√Æner et sauvegarder\n",
    "# trainer = ModelTrainer(model_type=\"gb\", target_col=\"target_empty\")\n",
    "# metrics = trainer.fit(df_train)\n",
    "# trainer.save(\"velib_model.joblib\")\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        # ==== Donn√©es d√©part (station de d√©part) ====\n",
    "        \"time\": \"2025-10-16T07:30:00Z\",   # Date/heure de d√©part (UTC)\n",
    "        \"station_id\": 75115034,           # ID station d√©part\n",
    "        \"capacity\": 40,                   # Capacit√© totale de la station\n",
    "        \"docks_available\": 24,           # Nombre de places libres\n",
    "        \"fill_rate\": 0.40,               # Ratio v√©los pr√©sents / capacit√©\n",
    "        \"rolling_fill_rate\": 0.38,       # Moyenne mobile du fill_rate\n",
    "        \"holiday_flag\": 0,              # 1 si jour f√©ri√©, sinon 0\n",
    "        \"is_weekend\": 0,               # 1 si week-end, sinon 0\n",
    "        \"operative\": 1,              # 1 si station op√©rationnelle, sinon 0\n",
    "\n",
    "        # # ==== Donn√©es arriv√©e (station d‚Äôarriv√©e) ====\n",
    "        # \"time\": \"2025-10-16T08:00:00Z\",     # Date/heure d‚Äôarriv√©e pr√©vue\n",
    "        # \"station_id_arrival\": 75104021,            # ID station arriv√©e\n",
    "        # \"capacity_arrival\": 25,                    # Capacit√© totale station arriv√©e\n",
    "        # \"docks_available_arrival\": 10,            # Nombre de places libres √† l‚Äôarriv√©e\n",
    "        # \"fill_rate_arrival\": 0.60,                # Ratio v√©los pr√©sents / capacit√©\n",
    "        # \"rolling_fill_rate_arrival\": 0.57,       # Moyenne mobile du fill_rate\n",
    "        # \"holiday_flag_arrival\": 0,              # 1 si jour f√©ri√©, sinon 0\n",
    "        # \"is_weekend_arrival\": 0,               # 1 si week-end, sinon 0\n",
    "        # \"operative_arrival\": 1,                # 1 si station op√©rationnelle, sinon 0\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Charger et pr√©dire\n",
    "# pred = Predictor(timezone=\"Europe/Paris\", target_col=\"target_empty\")\n",
    "# pred.load(\"velib_model.joblib\")\n",
    "# df_out = pred.predict_label(pd.DataFrame(data), threshold=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
